== START OF PROJECT CODEBASE DUMP ==
Project root: /Users/rob.hindhaugh/Documents/GitHub/Attendance_Dashboard

== DIRECTORY STRUCTURE (FILTERED) ==
- ./
  - README.md
  - code_writer.py
  - full_codebase.txt
  - main.py
  - requirements.txt
  - system_prompt.md
- ./tests/
  - test_data_cleaning.py
- ./data/
  - ./data/processed/
    - attendance_table.csv
    - avg_arrival_hours.csv
    - combined_data.csv
    - days_summary.csv
    - visit_counts.csv
  - ./data/raw/
    - employee_info.csv
    - key_card_access.csv
    - ./data/raw/csv_combiner/
      - csv_combiner.py
      - ./data/raw/csv_combiner/input_files/
- ./notebooks/
- ./src/
  - dashboard.py
  - data_analysis.py
  - data_cleaning.py
  - data_ingestion.py
  - data_visualization.py

== FILE CONTENTS (FILTERED) ==

==== START OF FILE: README.md ====
# Attendance Dashboard

## Description
This project provides an attendance dashboard for monitoring employee access.

## Installation
Installation instructions here.

## Usage
Usage instructions here.

==== END OF FILE: README.md ====

==== START OF FILE: code_writer.py ====
#!/usr/bin/env python3
"""
code_writer.py

Place this script in the root of your project. When run, it will generate a file
called 'full_codebase.txt' in the same folder. The output file contains:

1. A directory structure (folder/file hierarchy) for your code/data (excluding
   generic/system folders like .git, venv, node_modules, etc.).
2. The contents of each relevant file, prefaced by its path.
   - By default, we include .py, .csv, .txt, .md, .json, .yaml, .yml.
   - We skip other file types (feel free to adjust).
3. For CSV files, only the first 50 lines are included.
4. The result is a single text file you can upload to an LLM for context.

Usage:
    python code_writer.py
"""

import os

OUTPUT_FILENAME = "full_codebase.txt"
MAX_CSV_LINES = 50

# Directories to skip
EXCLUDED_DIRS = {
    ".git",
    "venv",
    "__pycache__",
    "node_modules",
    ".idea",
    ".vscode",
    ".cache"
}

# File extensions we want to include
ALLOWED_EXTENSIONS = {
    ".py",
    ".csv",
    ".txt",
    ".md",
    ".json",
    ".yaml",
    ".yml"
}

def main():
    project_root = os.path.abspath(".")
    with open(OUTPUT_FILENAME, "w", encoding="utf-8") as out_file:
        out_file.write("== START OF PROJECT CODEBASE DUMP ==\n")
        out_file.write(f"Project root: {project_root}\n\n")

        # 1) Write directory structure
        out_file.write("== DIRECTORY STRUCTURE (FILTERED) ==\n")
        for root, dirs, files in os.walk(project_root):
            # Filter out excluded directories
            dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]

            # Calculate indentation based on depth
            rel_path = os.path.relpath(root, project_root)
            level = rel_path.count(os.sep)
            indent = "  " * level
            short_root = "." if rel_path == "." else f".{os.sep}{rel_path}"
            out_file.write(f"{indent}- {short_root}/\n")

            # Filter files by allowed extensions
            for f in sorted(files):
                ext = os.path.splitext(f)[1].lower()
                if ext in ALLOWED_EXTENSIONS:
                    sub_indent = "  " * (level + 1)
                    out_file.write(f"{sub_indent}- {f}\n")

        out_file.write("\n== FILE CONTENTS (FILTERED) ==\n")

        # 2) Write the contents of each allowed file
        for root, dirs, files in os.walk(project_root):
            # Filter out excluded directories
            dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]

            for filename in sorted(files):
                ext = os.path.splitext(filename)[1].lower()
                if ext not in ALLOWED_EXTENSIONS:
                    continue  # Skip non-allowed file extensions

                file_path = os.path.join(root, filename)
                # Skip the output file itself
                if os.path.abspath(file_path) == os.path.abspath(os.path.join(project_root, OUTPUT_FILENAME)):
                    continue

                # Prepare relative path for display
                relative_path = os.path.relpath(file_path, project_root)

                out_file.write(f"\n==== START OF FILE: {relative_path} ====\n")

                # If it's a CSV, only read first 50 lines
                if ext == ".csv":
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            for i, line in enumerate(f):
                                if i >= MAX_CSV_LINES:
                                    out_file.write("... [Truncated after 50 lines]\n")
                                    break
                                out_file.write(line)
                    except Exception as e:
                        out_file.write(f"[Error reading CSV: {e}]\n")

                else:
                    # For allowed non-CSV files, read everything
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            contents = f.read()
                            out_file.write(contents)
                    except Exception as e:
                        out_file.write(f"[Error reading file: {e}]\n")

                out_file.write(f"\n==== END OF FILE: {relative_path} ====\n")

        out_file.write("\n== END OF PROJECT CODEBASE DUMP ==\n")

if __name__ == "__main__":
    main()

==== END OF FILE: code_writer.py ====

==== START OF FILE: main.py ====
from src.data_ingestion import load_key_card_data, load_employee_info
from src.data_cleaning import (
    clean_key_card_data,
    clean_employee_info,
    merge_key_card_with_employee_info,
    add_time_analysis_columns
)
from src.data_analysis import (
    build_attendance_table,
    calculate_visit_counts,
    calculate_average_arrival_hour
)

def main():
    """
    This function will:
    1. Load the key card data from data/raw/key_card_access.csv
    2. Load the employee info data from data/raw/employee_info.csv
    3. Clean both datasets
    4. Add time analysis columns
    5. Merge them
    6. Run attendance analysis
    7. Save results
    """

    # STEP 1: Load data
    key_card_df = load_key_card_data("data/raw/key_card_access.csv")
    print("Key card dataframe columns:", key_card_df.columns.tolist())
    print("\nFirst few rows of raw key card data:")
    print(key_card_df.head())

    employee_df = load_employee_info("data/raw/employee_info.csv")
    print("\nEmployee info columns:", employee_df.columns.tolist())
    print("\nFirst few rows of raw employee data:")
    print(employee_df.head())

    # STEP 2: Clean data
    key_card_df = clean_key_card_data(key_card_df)
    employee_df = clean_employee_info(employee_df)

    # STEP 3: Add time analysis columns
    key_card_df = add_time_analysis_columns(key_card_df)

    # STEP 4: Merge data
    combined_df = merge_key_card_with_employee_info(key_card_df, employee_df)

    # STEP 5: Print shapes / head of data
    print("\nKey card (clean) shape:", key_card_df.shape)
    print("Employee info (clean) shape:", employee_df.shape)
    print("Combined shape:", combined_df.shape)

    # STEP 6: Run attendance analysis
    attendance_table = build_attendance_table(combined_df)
    visit_counts = calculate_visit_counts(combined_df)
    avg_arrival_hours = calculate_average_arrival_hour(combined_df)

    # Print summary statistics
    print("\n=== ATTENDANCE SUMMARY ===")
    days_summary = (
        attendance_table[["employee_name", "days_attended"]]
        .drop_duplicates()
        .sort_values("days_attended", ascending=False)
    )
    print("\nTotal days attended by employee:")
    print(days_summary.head(10))  # Show top 10

    print("\nAverage arrival hours:")
    print(avg_arrival_hours.head(10))  # Show top 10

    # STEP 7: Save all results
    # Save combined data
    combined_df.to_parquet("data/processed/combined_data.parquet", index=False)
    combined_df.to_csv("data/processed/combined_data.csv", index=False)
    
    # Save analysis results
    attendance_table.to_csv("data/processed/attendance_table.csv", index=False)
    visit_counts.to_csv("data/processed/visit_counts.csv", index=False)
    avg_arrival_hours.to_csv("data/processed/avg_arrival_hours.csv", index=False)
    days_summary.to_csv("data/processed/days_summary.csv", index=False)

    print("\nAll data has been saved to the data/processed directory.")
    print("\nTo view the dashboard, run: streamlit run src/dashboard.py")

if __name__ == "__main__":
    main()

==== END OF FILE: main.py ====

==== START OF FILE: requirements.txt ====
pandas==2.0.0
numpy==1.24.0
jupyter==1.0.0
plotly==5.13.0
streamlit==1.20.0
pytest==7.3.1

==== END OF FILE: requirements.txt ====

==== START OF FILE: system_prompt.md ====
--------------------------------------------------------------------------------------------------------------------------------------------------------
THIS IS A INITIAL PROMPT FOR AN LLM TO GET THEM UP TO SPEED ON THE PROJECT. ALSO USEFUL FOR CURSOR AI AS CONTEXT
--------------------------------------------------------------------------------------------------------------------------------------------------------

SYSTEM PROMPT / SUMMARY
Project Context
You are assisting with a project to analyze office attendance data. The user has two CSV data sources:

Key card CSV (originally 100,000+ rows) with columns for date/time scans and user identifiers. Over the conversation, it evolved to have columns like employee_id, timestamp, and access_point.
Employee info CSV (800+ rows), containing detailed information about each employee (Employee #, status, hire date, department, etc.), which gets renamed to employee_id for merging.
The goal is to clean, merge, and analyze these datasets. The user also wants a Streamlit dashboard to visualize attendance trends (visit frequency, arrival times, etc.).

Directory Structure & Python Setup

A recommended repo layout was established, including data/raw/, data/processed/, src/ scripts (data_ingestion.py, data_cleaning.py, data_analysis.py, dashboard.py), plus optional notebooks/ and tests/.
The user created a virtual environment (with venv) and a requirements.txt specifying libraries (pandas, numpy, streamlit, plotly, etc.).
Data Handling Steps

Load CSVs: data_ingestion.py has functions like load_key_card_data(...) and load_employee_info(...).
Cleaning:
Key card data needed to parse date/time columns (day-first) and extract numeric IDs from strings (like "761 Clark Hemmings, Andre"). Rows without a valid numeric ID (e.g., cleaners) are dropped.
Employee info needed "Employee #" renamed to employee_id.
Merging: A function merges both DataFrames on employee_id.
Analysis: Simple functions in data_analysis.py calculate attendance stats (visit counts, average arrival hour).
Dashboard: A basic Streamlit app (dashboard.py) loads the data, cleans it, merges it, and shows tables/charts.
Key Issues Encountered & Their Solutions

ModuleNotFoundError: The user hadn’t installed pandas in the correct environment or had a malformed requirements.txt. Solution: ensure the correct environment is activated and the requirements.txt is correct.
Column mismatch (KeyError: 'Date/time'): The key card CSV actually had a column named "timestamp" instead of "Date/time". The solution was to match code references to the actual headers (or rename the CSV header).
Small Test File Instead of Real 100k Rows: The user saw only 100 rows and IDs like 1, 2, 3..., instead of 3-digit IDs. It turned out the CSV might be a test snippet, not the real data. The solution is to confirm the real file (with ~100k rows) is placed in data/raw/ and that the correct path is used.
NaNs after Merge: Some employee IDs in the key card data do not appear in the employee info CSV. Rows with missing matches lead to NaN values for employee info columns. If desired, an inner join or filtering out invalid IDs can remove those unmatched rows.
Important Instruction for the LLM
In any further conversation about this project:

Ask clarifying questions or request to see code/config files whenever something is ambiguous, especially if column names, file paths, or data shapes aren’t confirmed.
Do not rush to provide partial solutions based on assumptions—always confirm details if they are missing or unclear.
End of Summary

The key card data is only for the London office. However, all employees might scan in (e.g. employees from other offices might be visiting London).
Further, 

ADDENDUM: ADDITIONAL PROJECT CONTEXT
Employee Data Transformations
The project now incorporates several derived fields from employee data that are critical for accurate attendance analysis:

Status v2: Distinguishes between active employees, inactive employees, and upcoming hires
Combined hire date: Uses Original Hire Date when available, otherwise Hire Date
Most recent day worked: For inactive employees, uses Employment Status Date; for active employees, uses today's date
Tenure: Calculates employment duration based on hire and departure dates
Location group: Maps detailed office locations to broader categories (London, Kent, US, Other)
Ops or non-ops: Binary categorization of divisions
Year flags (2014-2025): Tracks which employees were active in each calendar year

Attendance Analysis Requirements
The attendance analysis has specific business requirements:

Focus on London+Hybrid employees: Primary attendance metrics target employees who are both:

London-based (Location = "London UK")
Hybrid workers (Working Status = "Hybrid")


Dynamic denominators for percentages: When calculating attendance percentages:

Must account for changing employee counts over time
Only include employees who were actually employed on a given date (between Combined hire date and Most recent day worked)
Different calculations needed for different time periods (weekday splits, division-specific metrics)


Dashboard structure: The dashboard should include:

Overview: Daily/weekly attendance percentages, weekday counts, division breakdowns
Daily view: Current-day attendance with London+Hybrid vs Other categories
Individual metrics: Per-employee stats with separate tracking for Mon/Fri vs Tues-Thurs attendance


Data segmentation needs: Analysis requires:

Monday/Friday vs Tuesday-Thursday comparison
Operation vs non-operations divisions
Location-based grouping
Tenure-based analysis



Technical Implementation Notes

Enhanced clean_employee_info() function in data_cleaning.py now handles all employee data transformations
New analysis functions in data_analysis.py implement the business logic for attendance calculations
Dashboard (dashboard.py) needs to display metrics with both raw numbers and percentage views where appropriate
All percentage calculations must use the appropriate dynamic denominator methods

The focus of the project has shifted from basic attendance tracking to sophisticated workforce analytics that account for employee lifecycle changes and provide accurate metrics for hybrid work attendance patterns.
==== END OF FILE: system_prompt.md ====

==== START OF FILE: tests/test_data_cleaning.py ====
# test_data_cleaning.py
# Add your code here

==== END OF FILE: tests/test_data_cleaning.py ====

==== START OF FILE: data/processed/attendance_table.csv ====
employee_id,employee_name,date_only,visits,present,days_attended
562.0,"Adegboye, Joy",2024-08-07,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-08,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-09,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-12,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-13,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-14,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-15,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-16,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-19,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-20,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-21,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-22,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-23,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-27,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-28,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-29,0.0,No,1.0
562.0,"Adegboye, Joy",2024-08-30,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-02,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-03,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-04,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-05,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-06,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-09,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-10,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-11,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-12,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-13,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-16,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-17,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-18,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-19,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-20,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-22,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-23,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-24,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-25,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-26,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-27,0.0,No,1.0
562.0,"Adegboye, Joy",2024-09-30,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-01,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-02,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-03,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-04,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-07,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-08,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-09,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-10,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-11,0.0,No,1.0
562.0,"Adegboye, Joy",2024-10-12,0.0,No,1.0
... [Truncated after 50 lines]

==== END OF FILE: data/processed/attendance_table.csv ====

==== START OF FILE: data/processed/avg_arrival_hours.csv ====
employee_id,arrival_hour
103.0,10.82
104.0,10.4
105.0,8.71
106.0,9.07
107.0,12.5
108.0,10.0
110.0,9.36
112.0,10.0
113.0,9.39
114.0,9.49
115.0,9.74
118.0,12.0
119.0,8.65
120.0,8.43
121.0,11.5
122.0,8.37
123.0,8.17
126.0,8.71
128.0,10.29
129.0,12.33
130.0,9.26
131.0,9.9
132.0,10.45
133.0,8.11
134.0,10.47
136.0,9.2
137.0,7.95
139.0,11.25
140.0,8.33
141.0,10.76
143.0,9.18
144.0,9.0
145.0,9.41
147.0,8.34
148.0,9.44
151.0,9.5
152.0,10.09
153.0,8.41
155.0,7.11
156.0,9.13
158.0,9.16
159.0,10.59
160.0,9.39
162.0,9.0
163.0,10.88
165.0,7.27
166.0,9.4
167.0,8.93
175.0,9.33
... [Truncated after 50 lines]

==== END OF FILE: data/processed/avg_arrival_hours.csv ====

==== START OF FILE: data/processed/combined_data.csv ====
timestamp,employee_id,User,date_only,day_of_week,time_only,earliest_scan_time,"Last name, First name",Status,Gender,Hire Date,Original Hire Date,Resignation Date,Working Status,Level,Employment Status: Date,Employment Status,FTE,Location,Division,Department,Job Title,Reporting to
2024-08-07 17:34:23,362,"362 Whitehead, Tom",2024-08-07,Wednesday,17:34:23,17:34:23,"Whitehead, Tom",Active,Male,20/09/2022,,,Hybrid,,20/09/2022,Full-Time,,London UK,Credit and Data,Credit & Collections,Senior Analyst,Calum Thompson
2024-08-07 17:35:39,364,"364 Keijzer, Tom",2024-08-07,Wednesday,17:35:39,17:35:39,"Keijzer, Tom",Active,Male,04/10/2022,,,Hybrid,,04/10/2022,Full-Time,,London UK,Credit and Data,Cards US,Senior Analyst,Mark Rosel
2024-08-07 17:35:49,288,"288 Snell, John",2024-08-07,Wednesday,17:35:49,17:35:49,"Snell, John",Inactive,Male,01/03/2022,,,Hybrid,,21/01/2025,Terminated,,London UK,Finance,Finance,Senior Accountant,Tiffany Victoria
2024-08-07 17:35:57,610,"610 Ike, Chinyere",2024-08-07,Wednesday,17:35:57,17:35:57,"Ike, Chinyere",Active,Female,15/11/2023,,,Hybrid,,03/06/2024,Full-Time,,London UK,Operations,Complaints,Complaints Officer,Tom Earley
2024-08-07 17:38:39,261,"261 Claudet, Christopher",2024-08-07,Wednesday,17:38:39,17:38:39,"Claudet, Christopher",Active,Male,11/01/2022,,,Remote,,11/01/2022,Full-Time,,London UK,Product,SuperApp,Senior Product Designer,Giulia Fabritius
2024-08-07 17:40:14,757,"757 Amicone, Floriana",2024-08-07,Wednesday,17:40:14,17:40:14,"Amicone, Floriana",Active,Female,09/07/2024,,,Hybrid,,09/07/2024,Full-Time,,London UK,Credit and Data,Data Science,Data Scientist,Oliver Cairns
2024-08-07 17:40:19,397,"397 Gordon, Richard",2024-08-07,Wednesday,17:40:19,17:40:19,"Gordon, Richard",Active,Male,14/02/2023,,,Hybrid,,14/02/2023,Full-Time,,London UK,Operations,Collections (Ops) UK,Collections Agent - Cards & Loans,Christian Porter
2024-08-07 17:40:20,310,"310 Davies, Thomas",2024-08-07,Wednesday,17:40:20,17:40:20,"Davies, Thomas",Active,Male,24/05/2022,,,Hybrid,,24/05/2022,Full-Time,,London UK,Product,SuperApp,Senior Product Manager,Giulia Fabritius
2024-08-07 17:40:55,304,"304 Fielding, Adam",2024-08-07,Wednesday,17:40:55,17:40:55,"Fielding, Adam",Active,Male,17/05/2022,,,Hybrid,IC3,29/07/2024,Full-Time,,London UK,Engineering,SuperApp,Senior Software Engineer,Nick Hannaway
2024-08-07 17:41:08,677,"677 Pratapa, Vamsi",2024-08-07,Wednesday,17:41:08,17:41:08,"Pratapa, Vamsi",Active,Male,12/03/2024,,,Hybrid,,12/03/2024,Full-Time,,London UK,Credit and Data,Operations Analytics,Graduate Analyst,Aakash Arora
2024-08-07 17:42:23,362,"362 Whitehead, Tom",2024-08-07,Wednesday,17:42:23,17:34:23,"Whitehead, Tom",Active,Male,20/09/2022,,,Hybrid,,20/09/2022,Full-Time,,London UK,Credit and Data,Credit & Collections,Senior Analyst,Calum Thompson
2024-08-07 17:42:48,745,"745 Leung, Joshua",2024-08-07,Wednesday,17:42:48,17:42:48,"Leung, Joshua",Active,Male,25/06/2024,,,Hybrid,IC3,29/07/2024,Full-Time,,London UK,Engineering,Cards UK,Senior React Native Engineer,Phillip Brown
2024-08-07 17:43:31,195,"195 Thompson, Calum",2024-08-07,Wednesday,17:43:31,17:43:31,"Thompson, Calum",Active,Male,14/06/2021,,,Hybrid,,14/06/2021,Full-Time,,London UK,Credit and Data,Credit & Collections,"Credit Risk Manager, Cards",Charles Denby
2024-08-07 17:43:52,790,"790 Chippendale, Julie",2024-08-07,Wednesday,17:43:52,17:43:52,"Chippendale, Julie",Active,Female,30/07/2024,,,Hybrid,,30/07/2024,Short-term Contractor,,London UK,Operations,Collections Strategy,Collections and Recoveries Consultant,Darren Carlile
2024-08-07 17:44:15,736,"736 Jameson, Dominnic",2024-08-07,Wednesday,17:44:15,17:44:15,"Jameson, Dominic",Active,Male,18/06/2024,,,Hybrid,,18/06/2024,Full-Time,,London UK,Capital Markets,Capital Markets,COO - Lendable Capital,Rory McHugh
2024-08-07 17:44:21,151,"151 Markland, Hugo",2024-08-07,Wednesday,17:44:21,17:44:21,"Markland, Hugo",Inactive,Male,09/12/2019,,,Hybrid,,23/10/2024,Terminated,,London UK,Capital Markets,Capital Markets,Director Capital Markets,Rory McHugh
2024-08-07 17:46:06,365,"365 Barratt-Johnson, Oliver",2024-08-07,Wednesday,17:46:06,17:46:06,"Barratt-Johnson, Oliver",Inactive,Male,04/10/2022,,19/12/2024,Hybrid,,28/01/2025,Terminated,,London UK,Credit and Data,Credit & Collections,Senior Credit Analyst,Calum Thompson
2024-08-07 17:48:27,259,"259 Ushanov, Mingiyan",2024-08-07,Wednesday,17:48:27,17:48:27,"Ushanov, Mingiyan",Active,Male,04/01/2022,,,Hybrid,,04/01/2022,Full-Time,,London UK,Growth,Growth,Senior Growth Analyst,Christopher Meurice
2024-08-07 17:48:30,264,"264 Anselmetti, Pietro",2024-08-07,Wednesday,17:48:30,17:48:30,"Anselmetti, Pietro",Active,Male,18/01/2022,,,Hybrid,,18/01/2022,Full-Time,,London UK,Growth,Growth,Senior UK Partnership Manager,Christopher Meurice
2024-08-07 17:50:09,317,"317 Soufla, Eirini",2024-08-07,Wednesday,17:50:09,17:50:09,"Soufla, Eirini",Active,Female,14/06/2022,,,Hybrid,,14/06/2022,Full-Time,,London UK,Operations,Customer Operations UK,Customer Operations Team Lead - Zable,Krishna Bhura
2024-08-07 17:50:18,343,"343 Wong, Christine",2024-08-07,Wednesday,17:50:18,17:50:18,"Wong, Christine",Active,Female,23/08/2022,,,Hybrid,,23/08/2022,Full-Time,,London UK,Operations,Collections (Ops) UK,Specialist Operations Agent,Steff Passon
2024-08-07 17:52:12,259,"259 Ushanov, Mingiyan",2024-08-07,Wednesday,17:52:12,17:48:27,"Ushanov, Mingiyan",Active,Male,04/01/2022,,,Hybrid,,04/01/2022,Full-Time,,London UK,Growth,Growth,Senior Growth Analyst,Christopher Meurice
2024-08-07 17:52:55,515,"515 Chan, Martin",2024-08-07,Wednesday,17:52:55,17:52:55,"Chan, Martin",Active,Male,15/08/2023,,,Hybrid,,15/08/2023,Full-Time,,London UK,CEO Office,CEO Office,VP Strategy & Corporate Development,Martin Kissinger
2024-08-07 17:53:17,304,"304 Fielding, Adam",2024-08-07,Wednesday,17:53:17,17:40:55,"Fielding, Adam",Active,Male,17/05/2022,,,Hybrid,IC3,29/07/2024,Full-Time,,London UK,Engineering,SuperApp,Senior Software Engineer,Nick Hannaway
2024-08-07 17:54:32,156,"156 White, James",2024-08-07,Wednesday,17:54:32,17:54:32,"White, James",Active,Male,10/02/2020,,,Hybrid,,10/02/2020,Full-Time,,London UK,Senior Management,Cards UK,"Managing Director, Cards UK",Martin Kissinger
2024-08-07 17:54:34,103,"103 Challis, Ben",2024-08-07,Wednesday,17:54:34,17:54:34,"Challis, Ben",Active,Male,08/06/2016,,,Hybrid,IC6,29/07/2024,Full-Time,,London UK,Engineering,Architecture,Chief Architect,James Caviness
2024-08-07 17:56:41,206,"206 Price, Matthew",2024-08-07,Wednesday,17:56:41,17:56:41,"Price, Matthew",Active,Male,19/07/2021,,,Hybrid,,19/07/2021,Full-Time,,London UK,Finance,Finance,Financial Operations Manager,Adam White
2024-08-07 17:58:38,661,"661 Murphy, David",2024-08-07,Wednesday,17:58:38,17:58:38,"Murphy, David",Active,Male,30/01/2024,,,Hybrid,IC4,29/07/2024,Full-Time,,London UK,Engineering,SuperApp,Tech Lead,Katherine Spice
2024-08-07 18:01:24,348,"348 Sampong, Emmanuel",2024-08-07,Wednesday,18:01:24,18:01:24,"Sampong, Emmanuel",Active,Male,06/09/2022,,,Hybrid,,06/09/2022,Full-Time,,London UK,Operations,QA,QA Agent,Scott Hick
2024-08-07 18:02:03,143,"143 Angopa, Bernard",2024-08-07,Wednesday,18:02:03,18:02:03,"Angopa, Bernard",Active,Male,08/07/2019,,,Hybrid,,08/07/2019,Full-Time,,London UK,Legal,Legal,General Counsel,Rory McHugh
2024-08-07 18:03:36,761,"761 Clarke-Hemmings, Andre",2024-08-07,Wednesday,18:03:36,18:03:36,"Clarke-Hemmings, Andre",Active,Male,09/07/2024,,,Hybrid,,09/07/2024,Full-Time,,London UK,Operations,Collections (Ops) UK,Collections Agent - Cards,Christian Porter
2024-08-07 18:04:12,106,"106 van Strydonck, Livia",2024-08-07,Wednesday,18:04:12,18:04:12,"van Strydonck, Livia",Active,Female,06/02/2017,,,Hybrid,,06/02/2017,Full-Time,,London UK,CEO Office,CEO Office,Chief of Staff,Martin Kissinger
2024-08-07 18:04:45,122,"122 Meurice, Christopher",2024-08-07,Wednesday,18:04:45,18:04:45,"Meurice, Christopher",Active,Male,18/09/2017,,,Hybrid,,18/09/2017,Full-Time,,London UK,Senior Management,Growth,Chief Marketing Officer,Martin Kissinger
2024-08-07 18:05:43,721,"721 Rahman, Rashidur",2024-08-07,Wednesday,18:05:43,18:05:43,"Rahman, Rashidur",Active,Male,04/06/2024,,,Hybrid,,04/06/2024,Full-Time,,London UK,Operations,Collections (Ops) UK,Collections Agent - Cards,Christian Porter
2024-08-07 18:06:15,686,"686 Steele, Madeleine",2024-08-07,Wednesday,18:06:15,18:06:15,"Steele, Madeleine",Active,Female,09/04/2024,,24/09/2024,Hybrid,,09/04/2024,Full-Time,,London UK,Compliance - UK,Compliance - UK,Compliance Monitoring Officer,Michelle Sharpley
2024-08-07 18:06:43,397,"397 Gordon, Richard",2024-08-07,Wednesday,18:06:43,17:40:19,"Gordon, Richard",Active,Male,14/02/2023,,,Hybrid,,14/02/2023,Full-Time,,London UK,Operations,Collections (Ops) UK,Collections Agent - Cards & Loans,Christian Porter
2024-08-07 18:06:45,126,"126 McGuire, Conor",2024-08-07,Wednesday,18:06:45,18:06:45,"McGuire, Conor",Active,Male,09/07/2018,,,Hybrid,,09/07/2018,Full-Time,,London UK,Senior Management,Compliance - UK,Head of Compliance,Martin Kissinger
2024-08-07 18:14:46,133,"133 White, Adam",2024-08-07,Wednesday,18:14:46,18:14:46,"White, Adam",Active,Male,07/01/2019,,,Hybrid,,07/01/2019,Full-Time,,London UK,Senior Management,Finance,"VP, Finance",Peter Golby
2024-08-07 18:20:58,365,"365 Barratt-Johnson, Oliver",2024-08-07,Wednesday,18:20:58,17:46:06,"Barratt-Johnson, Oliver",Inactive,Male,04/10/2022,,19/12/2024,Hybrid,,28/01/2025,Terminated,,London UK,Credit and Data,Credit & Collections,Senior Credit Analyst,Calum Thompson
2024-08-07 18:21:47,310,"310 Davies, Thomas",2024-08-07,Wednesday,18:21:47,17:40:20,"Davies, Thomas",Active,Male,24/05/2022,,,Hybrid,,24/05/2022,Full-Time,,London UK,Product,SuperApp,Senior Product Manager,Giulia Fabritius
2024-08-07 18:23:52,365,"365 Barratt-Johnson, Oliver",2024-08-07,Wednesday,18:23:52,17:46:06,"Barratt-Johnson, Oliver",Inactive,Male,04/10/2022,,19/12/2024,Hybrid,,28/01/2025,Terminated,,London UK,Credit and Data,Credit & Collections,Senior Credit Analyst,Calum Thompson
2024-08-07 18:26:34,319,"319 Prasad, Arnav",2024-08-07,Wednesday,18:26:34,18:26:34,"Prasad, Arnav",Active,Male,21/06/2022,,,Hybrid,,21/06/2022,Full-Time,,London UK,Credit and Data,Data Science,Data Science Manager,Kristian McCaul
2024-08-07 18:27:43,310,"310 Davies, Thomas",2024-08-07,Wednesday,18:27:43,17:40:20,"Davies, Thomas",Active,Male,24/05/2022,,,Hybrid,,24/05/2022,Full-Time,,London UK,Product,SuperApp,Senior Product Manager,Giulia Fabritius
2024-08-07 18:32:36,319,"319 Prasad, Arnav",2024-08-07,Wednesday,18:32:36,18:26:34,"Prasad, Arnav",Active,Male,21/06/2022,,,Hybrid,,21/06/2022,Full-Time,,London UK,Credit and Data,Data Science,Data Science Manager,Kristian McCaul
2024-08-07 18:42:23,103,"103 Challis, Ben",2024-08-07,Wednesday,18:42:23,17:54:34,"Challis, Ben",Active,Male,08/06/2016,,,Hybrid,IC6,29/07/2024,Full-Time,,London UK,Engineering,Architecture,Chief Architect,James Caviness
2024-08-07 18:47:46,537,"537 Kozulin, Aleksandr",2024-08-07,Wednesday,18:47:46,18:47:46,"Kozulin, Aleksandr",Active,Male,19/09/2023,,,Hybrid,,19/09/2023,Full-Time,,London UK,Credit and Data,Credit & Collections,Senior Credit Analyst,Steven Cochrane
2024-08-07 19:12:48,288,"288 Snell, John",2024-08-07,Wednesday,19:12:48,17:35:49,"Snell, John",Inactive,Male,01/03/2022,,,Hybrid,,21/01/2025,Terminated,,London UK,Finance,Finance,Senior Accountant,Tiffany Victoria
2024-08-07 19:15:26,288,"288 Snell, John",2024-08-07,Wednesday,19:15:26,17:35:49,"Snell, John",Inactive,Male,01/03/2022,,,Hybrid,,21/01/2025,Terminated,,London UK,Finance,Finance,Senior Accountant,Tiffany Victoria
2024-08-07 19:52:03,488,"488 de Grande, Esther",2024-08-07,Wednesday,19:52:03,19:52:03,"de Grande, Esther",Active,Female,27/06/2023,,,Hybrid,,27/06/2023,Full-Time,,London UK,People,People Operations,People & Talent Coordinator,Charlotte Mutsaerts
... [Truncated after 50 lines]

==== END OF FILE: data/processed/combined_data.csv ====

==== START OF FILE: data/processed/days_summary.csv ====
date,total_eligible,total_present,percentage
2024-08-07,264,36,13.6
2024-08-08,264,164,62.1
2024-08-09,263,21,8.0
2024-08-12,263,18,6.8
2024-08-13,263,185,70.3
2024-08-14,263,179,68.1
2024-08-15,263,169,64.3
2024-08-16,261,10,3.8
2024-08-19,261,19,7.3
2024-08-20,260,181,69.6
2024-08-21,259,175,67.6
2024-08-22,259,163,62.9
2024-08-23,259,13,5.0
2024-08-27,263,140,53.2
2024-08-28,263,126,47.9
2024-08-29,262,134,51.1
2024-08-30,262,11,4.2
2024-09-02,262,15,5.7
2024-09-03,264,180,68.2
2024-09-04,264,165,62.5
2024-09-05,264,170,64.4
2024-09-06,264,14,5.3
2024-09-09,264,16,6.1
2024-09-10,264,180,68.2
2024-09-11,264,177,67.0
2024-09-12,264,175,66.3
2024-09-13,264,11,4.2
2024-09-16,264,19,7.2
2024-09-17,265,199,75.1
2024-09-18,265,190,71.7
2024-09-19,265,177,66.8
2024-09-20,265,40,15.1
2024-09-22,265,4,1.5
2024-09-23,265,14,5.3
2024-09-24,266,194,72.9
2024-09-25,265,188,70.9
2024-09-26,265,190,71.7
2024-09-27,265,12,4.5
2024-09-30,265,13,4.9
2024-10-01,267,203,76.0
2024-10-02,267,192,71.9
2024-10-03,267,192,71.9
2024-10-04,267,16,6.0
2024-10-07,267,14,5.2
2024-10-08,269,204,75.8
2024-10-09,269,199,74.0
2024-10-10,269,191,71.0
2024-10-11,269,15,5.6
2024-10-12,269,1,0.4
... [Truncated after 50 lines]

==== END OF FILE: data/processed/days_summary.csv ====

==== START OF FILE: data/processed/visit_counts.csv ====
employee_id,visit_count
103,371
104,21
105,174
106,692
107,11
108,300
110,420
112,6
113,232
114,223
115,78
118,3
119,574
120,519
121,98
122,524
123,483
126,658
128,237
129,7
130,325
131,187
132,221
133,413
134,176
136,710
137,249
139,18
140,193
141,433
143,493
144,55
145,91
147,459
148,47
151,194
152,379
153,129
155,258
156,347
158,409
159,298
160,221
162,9
163,31
165,387
166,15
167,168
175,141
... [Truncated after 50 lines]

==== END OF FILE: data/processed/visit_counts.csv ====

==== START OF FILE: data/raw/employee_info.csv ====
﻿"Last name, First name","Employee #",Status,Gender,College/Institution,"Hire Date","Original Hire Date","Resignation Date","Working Status",Level,"Employment Status: Date","Employment Status",FTE,"Termination Type","Termination Reason","Eligible For Re-hire","Regrettable or Non-Regrettable","Employment status comments","Compensation: Date","Pay Schedule","Pay type","Pay rate","Pay rate - Currency code","Paid per","Overtime Status","Overtime Rate","Job Information: Date",Location,Division,Department,"Job Title","Reporting to",Entity,"Sick Leave Time taken (YTD)","Sick Leave Time accrued","Compassionate Leave Time accrued","Bereavement Leave Time accrued","Parental Leave Time accrued","Parental Leave Time taken (YTD)","Learning and Development Time taken (YTD)","Bereavement Leave Time taken (YTD)","Compassionate Leave Time taken (YTD)","Days in Lieu Time taken (YTD)","Work from Home Time taken (YTD)","SSP Sick Leave Time taken (YTD)","Unpaid Sick Leave Time taken (YTD)","OT - Intermediate Rate (Ops) Time taken (YTD)"
"Abdul, Alisha",717,Inactive,Female,,28/05/2024,,,Hybrid,,20/08/2024,Terminated,,,"End of contract",,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,08/07/2024,"Kent UK",Operations,"Customer Operations UK","Customer Operations Agent - Zable","Connor Delicata","Lendable UK",0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Abidoye, Olawale",395,Inactive,Male,,07/02/2023,,,Hybrid,,03/08/2023,Terminated,,"Resignation (Voluntary)",Relocation,Yes,Regrettable,"Moving to Jamaica for a new role. 
Resigned on 21st of July (2 weeks' notice in agreement with Krishna).",,,,,,,,,18/07/2023,"London UK",Operations,"Collections (Ops) UK","Collections Agent","Imren Iliyaz","Lendable UK",0.00,2.00,10.00,10.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Abraham, Daniel",716,Inactive,Male,,12/08/2024,28/05/2024,,Hybrid,,19/11/2024,Terminated,,"Termination (Involuntary)",Performance,No,,,30/09/2024,,Salary,0.01,GBP,Month,Exempt,,05/08/2024,"Kent UK",Operations,"Collections (Ops) UK","Collections Agent - Cards & Loans","Kimberley Radmore","Lendable UK",0.00,3.50,10.00,20.00,20.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adegboye, Joy",562,Active,Female,,17/10/2023,,,Hybrid,,17/10/2023,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,17/10/2023,"Kent UK",Operations,"Customer Operations UK","Customer Operations Agent - Zable","Connor Delicata","Lendable UK",1.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adèle, Amelia",236,Active,Female,,16/11/2021,,,Hybrid,,16/11/2021,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,13/01/2025,"London UK","Compliance - UK","Compliance - UK","Financial Crime Risk Manager","Scott Andrews","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adeleye, Jadesola",587,Active,Female,,07/11/2023,,,Hybrid,,07/11/2023,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,08/01/2025,"Kent UK",Operations,"Collections (Ops) UK","Collections Agent - Cards & Loans","Kimberley Radmore","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,3.00,0.00,1.00,2.00,0.00,0.00,0.00
"Ademiec, Radek",434,Active,Male,,18/04/2023,,,Remote,IC3,29/07/2024,"Long-term Contractor",,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,29/07/2024,Poland,Engineering,"Cards US","Senior Software Engineer","Arkadiusz Kondas","Lendable UK",0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adenekan, Taiwo",462,Inactive,Female,,30/05/2023,,,Hybrid,,29/08/2024,Terminated,,"Resignation (Voluntary)",Performance,"Upon review",Non-Regrettable,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,10/05/2024,"London UK",Operations,"Collections (Ops) UK","Collections Agent - Cards","Christian Porter","Lendable UK",0.00,7.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adeniji, Hannah",333,Inactive,Female,,02/08/2022,,10/05/2024,Hybrid,,30/05/2024,Terminated,,"Resignation (Voluntary)",Role,"Upon review",Non-Regrettable,,,,,,,,,,10/05/2024,"London UK",Operations,"Collections (Ops) UK","Collections Agent - Loans & Auto","Christian Porter","Lendable UK",0.00,2.55,4.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adewole, Zacchaeus",428,Inactive,Male,,11/04/2023,,,Hybrid,,22/08/2023,Terminated,,"Termination (Involuntary)",Performance,No,,"Failed QA 2 months in a row + concerns about behavioral issues. Decided to end his contract. One month notice ",,,,,,,,,12/05/2023,"London UK",Operations,"Customer Operations UK","Customer Operations Agent - Zable","Tom Earley","Lendable UK",0.00,8.00,10.00,10.00,10.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adeyemi, Nicole",709,Inactive,Female,,14/05/2024,,31/07/2024,Hybrid,,31/07/2024,Terminated,,"Resignation (Voluntary)",Role,No,Non-Regrettable,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,08/07/2024,"London UK",Operations,"Customer Operations UK","Customer Operations Agent - Zable","Eirini Soufla","Lendable UK",0.00,2.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adeyemo, Fawaz",670,Active,Male,,06/03/2024,,,Hybrid,,06/03/2024,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,15/10/2024,"London UK",Operations,"Customer Operations UK","Customer Operations Agent - Cards & Loans","Eirini Soufla","Lendable UK",0.63,8.00,5.00,10.00,10.00,0.00,0.00,0.00,0.00,1.00,2.00,0.00,0.00,0.00
"Adjei-Ampofo, Javan",936,Active,,,18/02/2025,,,Hybrid,,18/02/2025,Full-Time,,,,,,,,,,,,,,,18/02/2025,"Kent UK",Operations,"Customer Operations UK","Collections Agent - Loans","Christian Porter","Lendable UK",0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adjei, Chante",489,Inactive,Female,,27/06/2023,,,Hybrid,,27/09/2023,Terminated,,"Termination (Involuntary)",,,,,,,,,,,,,27/06/2023,"London UK",Operations,"Customer Operations UK","Customer Operations Agent - Loans","Jasmine Aminpour","Lendable UK",0.00,3.00,10.00,10.00,260.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Adofo, Johnson",755,Active,Male,,26/08/2024,,,Hybrid,,26/08/2024,Full-Time,,,,,,,27/08/2024,,Salary,0.01,GBP,Month,Exempt,,30/01/2025,"Kent UK",Operations,"Customer Operations UK","Customer Operations Agent - Zable","Kelly Smith","Lendable UK",0.00,8.00,5.00,10.00,10.00,0.00,0.00,0.00,0.00,1.00,2.00,0.00,0.00,0.00
"Afagwu-Odunowo, HappyPrincess",640,Active,Female,,05/12/2023,,,Hybrid,,05/12/2023,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,07/03/2024,"Kent UK",Operations,"Collections (Ops) UK","Collections Agent - Loans","Charlotte Harris","Lendable UK",0.00,8.00,5.00,8.00,154.00,0.00,0.00,0.00,0.00,1.00,0.00,0.00,0.00,0.00
"Afodunrinbi, Yetunde",864,Active,Female,,03/12/2024,,,Hybrid,,03/12/2024,Full-Time,,,,,,,03/12/2024,,Salary,0.10,GBP,Month,Exempt,,03/12/2024,"Kent UK",Operations,"Customer Operations UK","Customer Operations Agent - Auto","Jordan Nico","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Agbaje, David",758,Active,Male,,09/07/2024,,,Hybrid,,09/07/2024,Full-Time,,,,,,,10/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,09/07/2024,"Kent UK",Operations,"Collections (Ops) UK","Collections Agent - Cards","Charley Pratt","Lendable UK",0.00,8.00,5.00,10.00,10.00,0.00,0.00,0.00,0.00,1.00,0.00,0.00,0.00,0.00
"Agostinho Graça, Herberto",451,Inactive,Male,,22/05/2023,,,Remote,,07/03/2024,Terminated,,"Termination (Involuntary)",Performance,,,,,,,,,,,,01/01/2024,"The Netherlands",Engineering,"Central Platform","Staff Software Engineer","Phillip Brown","Lendable UK",0.00,8.00,5.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Agostinho, Lucelia",436,Inactive,Female,,18/04/2023,,,Hybrid,,13/02/2024,Terminated,,"Resignation (Voluntary)","Career Progression",Yes,,"- Wanted to find a role that had more chance of progression which she doesn't feel she has right now
- Some challenges with QA, concerned she would not pass her probation
",,,,,,,,,18/07/2023,"London UK",Operations,"Collections (Ops) UK","Collections Agent","Mandeep Kaur","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Agyemang-Duah, Nana",494,Inactive,Female,,18/07/2023,,02/05/2024,Hybrid,,30/05/2024,Terminated,,"Resignation (Voluntary)","Mental Health","Upon review",Regrettable,,,,,,,,,,20/11/2023,"London UK",Operations,"Customer Operations UK","Customer Operations Agent - Zable","Eirini Soufla","Lendable UK",0.00,0.69,1.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Ahmed, Faiza",731,Active,Female,,11/06/2024,,,Hybrid,,02/12/2024,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,19/11/2024,"Kent UK",Operations,"Customer Operations UK","Customer Operations Agent - Auto","Jordan Nico","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Ahmed, Sairah",496,Active,Female,,18/07/2023,,,Hybrid,,18/07/2023,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,22/12/2023,"London UK",Operations,"Customer Operations UK","Customer Operations Agent - Loans","George Thomas","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Ahmedi, Amtul Noor",401,Inactive,Female,,28/02/2023,,,Hybrid,,28/03/2024,Terminated,,"Resignation (Voluntary)","Personal reasons",No,Non-Regrettable,"Not aligned with how the business works",,,,,,,,,07/06/2023,"London UK",Operations,Complaints,"Head of Complaints","Claire Moore","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Aileru, Lynnex",590,Active,Male,,08/11/2023,,,Hybrid,,08/11/2023,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,28/10/2024,"Kent UK",Operations,Complaints,"Complaints Officer","Tom Earley","Lendable UK",0.00,8.00,5.00,10.00,10.00,0.00,0.00,0.00,0.00,0.00,1.00,0.00,0.00,0.00
"Akewushola, Mo",909,Active,Male,,15/01/2025,,,Hybrid,,15/01/2025,Temp,,,,,,,16/01/2025,,Salary,0.10,GBP,Month,Exempt,,15/01/2025,"London UK",Operations,"Fraud & FinCrime","Financial Crime Investigator","Alexandra Geca","Lendable UK",0.00,7.69,5.00,10.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Akhuemokhan, Esther A",895,Active,Female,,14/01/2025,,,,,14/01/2025,Temp,,,,,,,14/01/2025,,Salary,0.10,GBP,Month,Exempt,,14/01/2025,"Kent UK",Operations,"Customer Operations UK","Customer Operations Agent - Zable","Connor Delicata",,0.00,7.72,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Akintokunbo, Modupe",888,Active,Female,,14/01/2025,,,Hybrid,,14/01/2025,Full-Time,,,,,,,14/01/2025,,Salary,0.10,GBP,Month,Exempt,,14/01/2025,"Kent UK",Operations,"Collections (Ops) UK","Collections Agent - Loans","Kimberley Radmore","Lendable UK",0.00,7.72,5.00,10.00,154.00,0.00,0.00,1.00,3.50,0.00,0.00,0.00,0.00,0.00
"Alam, Rifat",672,Active,Male,,21/05/2024,,,Hybrid,IC2,29/07/2024,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,04/02/2025,"London UK",Engineering,SuperApp,"React Native Software Engineer","Nick Hannaway","Lendable UK",0.00,8.00,5.00,10.00,10.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Alanna, Montana",700,Inactive,Female,,07/05/2024,,14/05/2024,Hybrid,,14/05/2024,Terminated,,"Resignation (Voluntary)",culture,No,Non-Regrettable,,,,,,,,,,07/05/2024,"Kent UK",Operations,"Customer Operations UK","Customer Operations Agent - Zable","Connor Delicata","Lendable UK",0.00,8.00,5.00,10.00,10.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Alexander, Claire",572,Inactive,,,24/10/2023,,,Hybrid,,01/11/2023,Terminated,,"Resignation (Voluntary)",Other,,,,,,,,,,,,24/10/2023,"London UK",Operations,Complaints,"Complaints Officer","Tom Earley","Lendable UK",0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Ali, Obaid",367,Inactive,Male,,04/10/2022,,,Hybrid,,29/03/2023,Terminated,,"Termination (Involuntary)",Performance,,,,,,,,,,,,10/10/2022,"London UK",Operations,"Customer Operations UK","Customer Operations Agent - Zable","Tom Earley","Lendable UK",0.00,8.00,10.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Allen, Leanne",473,Active,Female,,06/06/2023,,,Hybrid,,06/06/2023,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,06/06/2023,"Kent UK",Operations,"Collections (Ops) UK","Specialist Vulnerability Team Lead","Daniel Medcalf","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,1.00,0.00,0.00,0.00,0.00
"Allison, Donna",550,Active,Female,,03/10/2023,,,Hybrid,,03/10/2023,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,11/09/2024,"Kent UK",Operations,"Collections (Ops) UK","Specialist Vulnerability Agent - Cards and Loans","Leanne Allen","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,1.00,3.00,0.00,0.00,0.00
"Allotey, Sharon",691,Inactive,Female,,23/04/2024,,,Hybrid,,06/12/2024,Terminated,,"Termination (Involuntary)",Performance,No,,"mutual agreement",04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,23/04/2024,"London UK",Operations,Complaints,"Complaints Officer","Tom Earley","Lendable UK",0.00,0.00,5.00,10.00,60.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Alokolaro, Elizabeth",230,Active,Female,,26/10/2021,,,Remote,,26/10/2021,"Short-term Contractor",,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,13/01/2025,"London UK","Compliance - UK","Compliance - UK","Financial Crime Specialist","Amelia Adèle","Lendable UK",0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Amicone, Floriana",757,Active,Female,,09/07/2024,,,Hybrid,,09/07/2024,Full-Time,,,,,,,10/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,09/07/2024,"London UK","Credit and Data","Data Science","Data Scientist","Oliver Cairns","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Aminpour, Jasmine",200,Active,Female,,28/06/2021,,,Hybrid,,28/06/2021,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,13/01/2025,"London UK","Compliance - UK","Compliance - UK","Compliance Monitoring Officer","Michelle Sharpley","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Amodubello, Sinead",740,Inactive,Female,,18/06/2024,,,Hybrid,,20/08/2024,Terminated,,,"End of contract",,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,08/07/2024,"Kent UK",Operations,"Customer Operations UK","Customer Operations Agent - Cards & Loans","Connor Delicata","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Anderson, Jake",556,Inactive,Male,,03/10/2023,,,Hybrid,,26/03/2024,Terminated,,"Resignation (Voluntary)","Personal reasons","Upon review",Non-Regrettable,,,,,,,,,,07/03/2024,"Kent UK",Operations,"Collections (Ops) UK","Collections Agent - Loans","Charlotte Harris","Lendable UK",0.00,7.00,2.00,10.00,10.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Andrade, Nzingha",495,Active,Female,,18/07/2023,,,Hybrid,,18/07/2023,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,01/01/2025,"London UK",Operations,"Fraud & FinCrime","Fraud & Disputes Advisor","Garry Rayment","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,0.00,2.00,0.00,0.00,0.00
"Andrews, Scott",192,Active,Male,,18/05/2021,,,Hybrid,,18/05/2021,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,13/01/2025,"London UK","Compliance - UK","Compliance - UK","Director of Enterprise Risk","Conor McGuire","Lendable UK",0.00,8.00,5.00,10.00,10.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Angopa, Bernard",143,Active,Male,,08/07/2019,,,Hybrid,,08/07/2019,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,08/07/2019,"London UK",Legal,Legal,"General Counsel","Rory McHugh","Lendable UK",0.00,8.00,5.00,10.00,10.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Anguelov, Nikolay",157,Inactive,Male,,17/02/2020,,,Hybrid,,28/02/2022,Terminated,,,Other,,,,,,,,,,,,17/02/2020,"London UK","Credit, Data and FinCrime","Collections & Internal Tools","Software Engineer","William Janse van Rensburg","Lendable UK",0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00
"Anidugbe, Olasubomi",763,Active,Male,,09/07/2024,,,Hybrid,,09/07/2024,Full-Time,,,,,,,10/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,09/07/2024,"London UK",Operations,"Customer Operations UK","Customer Operations Agent - Loans","George Thomas","Lendable UK",0.00,8.00,5.00,10.00,10.00,0.00,0.00,0.00,0.00,1.00,2.00,0.00,0.00,0.00
"Anscomb, Kylie",471,Active,Female,,06/06/2023,,,Hybrid,,06/06/2023,Full-Time,,,,,,,04/07/2024,,Salary,0.01,GBP,Month,Non-exempt,0.01,06/05/2024,"Kent UK",Operations,"Collections (Ops) UK","Senior Collections Agent - Loans & Auto","Danielle Munson","Lendable UK",0.00,8.00,5.00,10.00,154.00,0.00,0.00,0.00,0.00,2.00,0.00,0.00,0.00,0.00
... [Truncated after 50 lines]

==== END OF FILE: data/raw/employee_info.csv ====

==== START OF FILE: data/raw/key_card_access.csv ====
"Date/time","User","Details","Event","Department","Where"
"24/01/2025 23:27:47","CLEANER Cesar, Cleaner","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 23:14:32","CLEANER Cesar, Cleaner","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 20:51:42","CLEANER Cesar, Cleaner","","Access permitted - token only","BambooHR","Lift Lobby South (In)"
"24/01/2025 20:38:41","No Info Allauca, Miguel","","Access permitted - token only","BambooHR","Lift Lobby South (In)"
"24/01/2025 20:00:25","No Info Allauca, Miguel","","Access permitted - token only","BambooHR","Lift Lobby South (In)"
"24/01/2025 19:20:45","801 Purll, Laura","","Access permitted - token only","BambooHR","Lift Lobby South (In)"
"24/01/2025 18:36:06","No Info Allauca, Miguel","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 18:23:18","761 Clark Hemmings, Andre","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 18:20:00","761 Clark Hemmings, Andre","","Access permitted - token only","BambooHR","Office Entrance West (In)"
"24/01/2025 17:55:07","597 Chan, Aaron","","Access permitted - token only","BambooHR","5th Floor RH Side Entrance  (In)"
"24/01/2025 17:49:34","890 Capello, Giulio","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 17:43:27","440 Khan, Shahed","","Access permitted - token only","BambooHR","5th Floor RH Side Entrance  (In)"
"24/01/2025 17:41:44","440 Khan, Shahed","","Access permitted - token only","BambooHR","Office Entrance West (In)"
"24/01/2025 17:38:58","CLEANER Cleaner, Analuisa, Maria","","Access permitted - token only","BambooHR","Lift Lobby South (In)"
"24/01/2025 17:38:22","CLEANER Cleaner, Analuisa, Maria","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 17:33:05","190 van Lennep, Harold","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 17:20:59","CLEANER Cleaner, Analuisa, Maria","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 17:10:54","CLEANER Cleaner, Analuisa, Maria","","Access permitted - token only","BambooHR","Lift Lobby South (In)"
"24/01/2025 17:02:36","440 Khan, Shahed","","Access permitted - token only","BambooHR","5th Floor RH Side Entrance  (In)"
"24/01/2025 16:48:24","143 Angopa, Bernard","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 16:44:05","701 Szigeti, Ignac","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 16:33:39","597 Chan, Aaron","","Access permitted - token only","BambooHR","5th Floor RH Side Entrance  (In)"
"24/01/2025 16:28:46","440 Khan, Shahed","","Access permitted - token only","BambooHR","5th Floor RH Side Entrance  (In)"
"24/01/2025 16:26:29","440 Khan, Shahed","","Access permitted - token only","BambooHR","2nd Floor South RH Side (In)"
"24/01/2025 16:23:57","907 Higgins, Tom","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 16:21:18","907 Higgins, Tom","","Access permitted - token only","BambooHR","Lift Lobby South (In)"
"24/01/2025 15:51:52","673 Latham, Milo","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 15:31:56","190 van Lennep, Harold","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 15:30:53","190 van Lennep, Harold","","Access permitted - token only","BambooHR","Lift Lobby South (In)"
"24/01/2025 15:29:23","538 Hughes, Alexander","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 15:27:22","498 Cairns, Oliver","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 15:11:13","890 Capello, Giulio","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 15:10:04","890 Capello, Giulio","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 15:08:37","890 Capello, Giulio","","Access permitted - token only","BambooHR","Lift Lobby South (In)"
"24/01/2025 15:03:08","597 Chan, Aaron","","Access permitted - token only","BambooHR","5th Floor RH Side Entrance  (In)"
"24/01/2025 15:01:14","673 Latham, Milo","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 14:43:08","801 Purll, Laura","","Access permitted - token only","BambooHR","Office Entrance West (In)"
"24/01/2025 14:41:09","668 Stone, William","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 14:32:00","190 van Lennep, Harold","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 14:22:18","264 Anselmetti, Pietro","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 14:16:18","440 Khan, Shahed","","Access permitted - token only","BambooHR","5th Floor RH Side Entrance  (In)"
"24/01/2025 14:15:04","440 Khan, Shahed","","Access permitted - token only","BambooHR","Office Entrance West (In)"
"24/01/2025 14:12:43","440 Khan, Shahed","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 14:09:20","689 Suman, Anushka","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 13:57:14","190 van Lennep, Harold","","Access permitted - token only","BambooHR","Office Entrance West (In)"
"24/01/2025 13:52:39","597 Chan, Aaron","","Access permitted - token only","BambooHR","5th Floor RH Side Entrance  (In)"
"24/01/2025 13:48:32","701 Szigeti, Ignac","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
"24/01/2025 13:46:56","907 Higgins, Tom","","Access permitted - token only","BambooHR","Lift Lobby North (In)"
"24/01/2025 13:40:24","143 Angopa, Bernard","","Access permitted - token only","BambooHR","2nd Floor South LH Side (In)"
... [Truncated after 50 lines]

==== END OF FILE: data/raw/key_card_access.csv ====

==== START OF FILE: data/raw/csv_combiner/csv_combiner.py ====
#!/usr/bin/env python3
import os
import glob
import pandas as pd

def combine_csv_files(input_dir, output_file, subset_cols=None):
    """
    Combines all CSV files in input_dir into one DataFrame,
    removes duplicate rows, and saves the result to output_file.
    
    Parameters:
      - input_dir: str, directory where the CSV files are located.
      - output_file: str, path to the output CSV file.
      - subset_cols: list of str, optional. If provided, duplicates will be
                     dropped based on these columns.
    """
    # Find all CSV files in the input directory
    csv_files = glob.glob(os.path.join(input_dir, "*.csv"))
    if not csv_files:
        print(f"No CSV files found in {input_dir}")
        return

    # Read each CSV file into a DataFrame
    df_list = []
    for file in csv_files:
        try:
            df = pd.read_csv(file)
            df_list.append(df)
            print(f"Loaded {file} with {len(df)} rows")
        except Exception as e:
            print(f"Error reading {file}: {e}")

    # Concatenate all DataFrames
    combined_df = pd.concat(df_list, ignore_index=True)
    print(f"\nCombined DataFrame has {len(combined_df)} rows before deduplication.")

    # Drop duplicates (using all columns or a subset if provided)
    if subset_cols:
        combined_df = combined_df.drop_duplicates(subset=subset_cols)
    else:
        combined_df = combined_df.drop_duplicates()
    print(f"Combined DataFrame has {len(combined_df)} rows after deduplication.")

    # Save the combined DataFrame to the output CSV file
    combined_df.to_csv(output_file, index=False)
    print(f"\nCombined CSV saved to {output_file}")

if __name__ == "__main__":
    # Define the input directory (where individual CSV files are stored)
    input_directory = os.path.join("data", "raw", "csv_combiner", "input_files")
    
    # Define the output CSV path, which is the file your project currently uses.
    output_csv = os.path.join("data", "raw", "key_card_access.csv")
    
    # Optionally define columns to use for deduplication.
    # For example, if 'Date/time' and 'User' uniquely identify a record:
    dedupe_subset = ['Date/time', 'User']
    
    combine_csv_files(input_directory, output_csv, subset_cols=dedupe_subset)

==== END OF FILE: data/raw/csv_combiner/csv_combiner.py ====

==== START OF FILE: src/dashboard.py ====
import streamlit as st
import pandas as pd
import plotly.express as px
from pathlib import Path

# Data cleaning and ingestion imports
from data_cleaning import (
    load_key_card_data,
    load_employee_info,
    clean_key_card_data,
    clean_employee_info,
    merge_key_card_with_employee_info,
    add_time_analysis_columns
)

# Data analysis imports
from data_analysis import (
    build_attendance_table,
    calculate_visit_counts,
    calculate_average_arrival_hour,
    calculate_daily_attendance_percentage,
    calculate_weekly_attendance_percentage,
    calculate_attendance_by_weekday,
    calculate_attendance_by_division,
    calculate_individual_attendance,
    create_employee_summary,
    calculate_tue_thu_attendance_percentage,
    calculate_daily_attendance_counts,
    calculate_weekly_attendance_counts,
    calculate_period_summary
)

def load_data():
    """Load and preprocess the data."""
    # Load key card data
    key_card_path = Path("data/raw/key_card_access.csv")
    key_card_df = pd.read_csv(key_card_path)
    print("\nLoaded key card data columns:", key_card_df.columns.tolist())
    
    # Load employee info
    employee_path = Path("data/raw/employee_info.csv")
    employee_df = pd.read_csv(employee_path)
    print("\nLoaded employee info columns:", employee_df.columns.tolist())
    
    return key_card_df, employee_df

def save_processed_data(attendance_table, daily_attendance_pct, avg_arrival_hours):
    """Save processed data to CSV files."""
    # Create processed data directory if it doesn't exist
    processed_dir = Path("data/processed")
    processed_dir.mkdir(parents=True, exist_ok=True)
    
    # Save each DataFrame
    attendance_table.to_csv(processed_dir / "attendance_table.csv", index=False)
    daily_attendance_pct.to_csv(processed_dir / "days_summary.csv", index=False)
    avg_arrival_hours.to_csv(processed_dir / "avg_arrival_hours.csv", index=False)

def load_and_process_data():
    """Load and process all data, returning the combined DataFrame."""
    # Load raw data
    key_card_df, employee_df = load_data()
    
    # Clean data
    key_card_df = clean_key_card_data(key_card_df)
    employee_df = clean_employee_info(employee_df)
    
    # Merge datasets
    combined_df = merge_key_card_with_employee_info(key_card_df, employee_df)
    
    # Create attendance table to get 'present' field
    attendance_table = build_attendance_table(combined_df)
    
    # Merge attendance data back to combined_df
    combined_df = combined_df.merge(
        attendance_table[['employee_id', 'date_only', 'present', 'visits']],
        on=['employee_id', 'date_only'],
        how='left'
    )
    
    # Fill any missing values in present column with 'No'
    combined_df['present'] = combined_df['present'].fillna('No')
    
    # Print debug info
    print("\nProcessed data info:")
    print(f"Total rows: {len(combined_df)}")
    print(f"Rows with present=Yes: {(combined_df['present'] == 'Yes').sum()}")
    print(f"Unique employees: {combined_df['employee_id'].nunique()}")
    print("\nSample of processed data:")
    print(combined_df[['employee_id', 'date_only', 'present', 'visits', 'Location', 'Working Status']].head())
    
    return combined_df

def filter_by_date_range(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp, date_col: str = 'date') -> pd.DataFrame:
    """Filter DataFrame by date range."""
    return df[
        (df[date_col] >= pd.Timestamp(start_date)) &
        (df[date_col] <= pd.Timestamp(end_date))
    ]

def format_date(date_str):
    """Convert date string to formatted date (e.g., '7th August 2024')"""
    date = pd.to_datetime(date_str)
    
    def ordinal(n):
        if 10 <= n % 100 <= 20:
            suffix = 'th'
        else:
            suffix = {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')
        return str(n) + suffix
    
    return f"{ordinal(date.day)} {date.strftime('%B %Y')}"

def main():
    """Main function to run the dashboard."""
    st.title("Office Attendance Dashboard")
    
    try:
        # Load and process data
        combined_df = load_and_process_data()
        
        # Create analyses
        tue_thu_attendance = calculate_tue_thu_attendance_percentage(combined_df)
        daily_counts = calculate_daily_attendance_counts(combined_df)
        weekly_counts = calculate_weekly_attendance_counts(combined_df)
        
        # Date range selector in sidebar
        st.sidebar.header("Date Range Selection")
        min_date = combined_df['date_only'].min()
        max_date = combined_df['date_only'].max()
        
        default_start = min_date
        default_end = max_date
        
        # Quick selection buttons
        st.sidebar.subheader("Quick Select")
        if st.sidebar.button("Last 30 Days"):
            default_start = max_date - pd.Timedelta(days=30)
            default_end = max_date
        if st.sidebar.button("Last 90 Days"):
            default_start = max_date - pd.Timedelta(days=90)
            default_end = max_date
        
        selected_dates = st.sidebar.date_input(
            "Select Date Range",
            value=(default_start, default_end),
            min_value=min_date,
            max_value=max_date
        )
        
        # Ensure we have both start and end dates
        if len(selected_dates) == 2:
            # Convert date objects to timestamps
            start_date = pd.Timestamp(selected_dates[0])
            end_date = pd.Timestamp(selected_dates[1])
            
            # Filter all DataFrames
            filtered_tue_thu = filter_by_date_range(tue_thu_attendance, start_date, end_date)
            filtered_daily = filter_by_date_range(daily_counts, start_date, end_date)
            filtered_weekly = filter_by_date_range(weekly_counts, start_date, end_date, 'week_start')
            
            # Calculate period summary
            period_summary = calculate_period_summary(combined_df, start_date, end_date)
            
            # Create tabs
            tab1, tab2, tab3, tab4 = st.tabs(["Daily Overview", "Weekly Overview", "Period Summary", "Employee Details"])
            
            with tab1:
                st.subheader("Daily Attendance Percentage (Tuesday-Thursday)")
                if len(filtered_tue_thu) > 0:
                    fig_daily_pct = px.line(
                        filtered_tue_thu,
                        x='date',
                        y='percentage',
                        title='Daily Office Attendance Percentage (Tue-Thu)',
                        labels={'percentage': 'Attendance %', 'date': 'Date'}
                    )
                    st.plotly_chart(fig_daily_pct)
                
                st.subheader("Daily Attendance Counts")
                if len(filtered_daily) > 0:
                    fig_daily_counts = px.bar(
                        filtered_daily,
                        x='date',
                        y=['london_hybrid_count', 'other_count'],
                        title='Daily Attendance by Employee Type',
                        labels={
                            'date': 'Date',
                            'value': 'Number of Employees',
                            'variable': 'Employee Type'
                        },
                        barmode='stack'
                    )
                    
                    fig_daily_counts.update_traces(
                        name='London + Hybrid',
                        selector=dict(name='london_hybrid_count')
                    )
                    fig_daily_counts.update_traces(
                        name='Other Employees',
                        selector=dict(name='other_count')
                    )
                    
                    st.plotly_chart(fig_daily_counts)
                
                # Daily details table
                st.subheader("Daily Attendance Details")
                
                # Format the date column before display
                display_df = filtered_daily.copy()
                display_df['date'] = display_df['date'].apply(format_date)
                
                # Rename columns to be more readable
                column_mapping = {
                    'date': 'Date',
                    'day_of_week': 'Day of Week',
                    'london_hybrid_count': 'London + Hybrid Present',
                    'other_count': 'Other Employees Present',
                    'eligible_london_hybrid': 'Total Eligible London + Hybrid',
                    'london_hybrid_percentage': 'London + Hybrid Attendance %',
                    'total_attendance': 'Total Attendance'
                }
                
                display_df = display_df.rename(columns=column_mapping)
                st.dataframe(display_df, hide_index=True)
            
            with tab2:
                st.subheader("Weekly Attendance Percentage")
                if len(filtered_weekly) > 0:
                    fig_weekly_pct = px.line(
                        filtered_weekly,
                        x='week_start',
                        y='london_hybrid_percentage',
                        title='Weekly Office Attendance Percentage',
                        labels={
                            'london_hybrid_percentage': 'Attendance %',
                            'week_start': 'Week Starting'
                        }
                    )
                    st.plotly_chart(fig_weekly_pct)
                
                st.subheader("Weekly Attendance Counts")
                if len(filtered_weekly) > 0:
                    fig_weekly_counts = px.bar(
                        filtered_weekly,
                        x='week_start',
                        y=['london_hybrid_avg', 'other_avg'],
                        title='Average Daily Attendance by Employee Type (Weekly)',
                        labels={
                            'week_start': 'Week Starting',
                            'value': 'Average Daily Attendance',
                            'variable': 'Employee Type'
                        },
                        barmode='stack'
                    )
                    
                    fig_weekly_counts.update_traces(
                        name='London + Hybrid',
                        selector=dict(name='london_hybrid_avg')
                    )
                    fig_weekly_counts.update_traces(
                        name='Other Employees',
                        selector=dict(name='other_avg')
                    )
                    
                    st.plotly_chart(fig_weekly_counts)
                
                # Weekly details table
                st.subheader("Weekly Attendance Details")
                display_cols_weekly = {
                    'week_start': 'Week Starting',
                    'london_hybrid_avg': 'Avg. London + Hybrid Count',
                    'other_avg': 'Avg. Other Count',
                    'london_hybrid_percentage': 'London + Hybrid %',
                    'total_avg_attendance': 'Total Avg. Attendance'
                }
                weekly_display = filtered_weekly[display_cols_weekly.keys()].rename(columns=display_cols_weekly)
                st.dataframe(weekly_display, hide_index=True)
            
            with tab3:
                st.subheader("Period Summary")
                if len(period_summary) > 0:
                    fig_period = px.bar(
                        period_summary,
                        x='weekday',
                        y=['london_hybrid_count', 'other_count'],
                        title='Average Daily Attendance by Weekday',
                        labels={
                            'weekday': 'Day of Week',
                            'value': 'Average Attendance',
                            'variable': 'Employee Type'
                        },
                        barmode='group'
                    )
                    
                    fig_period.update_traces(
                        name='London + Hybrid',
                        selector=dict(name='london_hybrid_count')
                    )
                    fig_period.update_traces(
                        name='Other Employees',
                        selector=dict(name='other_count')
                    )
                    
                    st.plotly_chart(fig_period)
                    
                    # Period summary table
                    st.subheader("Weekday Averages")
                    display_cols_period = {
                        'weekday': 'Day of Week',
                        'london_hybrid_count': 'Avg. London + Hybrid Count',
                        'other_count': 'Avg. Other Count',
                        'attendance_percentage': 'London + Hybrid Attendance %'
                    }
                    period_display = period_summary[display_cols_period.keys()].rename(columns=display_cols_period)
                    st.dataframe(period_display, hide_index=True)
            
            with tab4:
                st.subheader("Employee Attendance Summary")
                
                # Filter employee summary for selected date range
                date_filtered_df = combined_df[
                    (combined_df['date_only'] >= start_date) &
                    (combined_df['date_only'] <= end_date)
                ]
                
                filtered_employee_summary = create_employee_summary(date_filtered_df)
                st.dataframe(filtered_employee_summary, hide_index=True)
        
        else:
            st.error("Please select both start and end dates")
            
    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        print(f"Error details: {e}")

if __name__ == "__main__":
    main()

==== END OF FILE: src/dashboard.py ====

==== START OF FILE: src/data_analysis.py ====
import pandas as pd

def build_attendance_table(df: pd.DataFrame) -> pd.DataFrame:
    """
    Build attendance table from key card data.
    """
    df = df.copy()
    
    # Debug step 1: Show earliest 3 scans per day per employee
    print("\n[DEBUG] Earliest 3 scans per day per employee:")
    temp = (
        df
        .sort_values(["employee_id", "date_only", "parsed_time"])
        .groupby(["employee_id", "date_only"])
        .head(3)
    )
    print(temp[["employee_id", "date_only", "parsed_time", "Where"]].head(50))
    
    # Debug step 3: Check location/working status filtering
    location_mask = (df["Location"] == "London UK")
    working_mask = (df["Working Status"] == "Hybrid")
    print("\n[DEBUG] Location/Working Status Analysis:")
    print(f"Total rows: {len(df)}")
    print(f"Rows with non-London or non-Hybrid: {len(df[~(location_mask & working_mask)])}")
    
    # Group value counts for investigation
    print("\nLocation distribution:")
    print(df["Location"].value_counts())
    print("\nWorking Status distribution:")
    print(df["Working Status"].value_counts())
    
    # 1) Get unique employees and dates
    unique_employees = df[["employee_id", "Last name, First name"]].drop_duplicates()
    unique_dates = df["date_only"].unique()
    
    # 2) Build cartesian product (every employee x every date)
    employee_dates = []
    for _, employee in unique_employees.iterrows():
        for date in unique_dates:
            employee_dates.append({
                "employee_id": employee["employee_id"],
                "employee_name": employee["Last name, First name"],
                "date_only": date
            })
    
    cross_df = pd.DataFrame(employee_dates)
    
    # 3) Mark presence by checking if there's data for that employee-date
    attendance = (
        df.groupby(["employee_id", "date_only"])
        .size()
        .reset_index(name="visits")
    )
    
    # Merge attendance data with our cartesian product
    merged = cross_df.merge(
        attendance,
        on=["employee_id", "date_only"],
        how="left"
    )
    
    # After marking presence
    merged["visits"] = merged["visits"].fillna(0)
    merged["present"] = merged["visits"].map({0: "No"}).fillna("Yes")
    
    # Debug step 2: Show rows marked as 'present'
    print("\n[DEBUG] Checking presence logic. Sample 'present' rows:")
    present_only = merged[merged["present"] == "Yes"]
    print(present_only[["employee_id", "employee_name", "date_only", "present", "visits"]].head(30))
    
    # 4) Calculate total days attended per employee
    days_attended = (
        merged[merged["present"] == "Yes"]
        .groupby("employee_id")
        .size()
        .reset_index(name="days_attended")
    )
    
    # Merge days_attended back to our main table
    final_df = merged.merge(days_attended, on="employee_id", how="left")
    
    # Sort by employee name and date
    final_df = final_df.sort_values(["employee_name", "date_only"])
    
    return final_df

def calculate_visit_counts(df: pd.DataFrame) -> pd.DataFrame:
    """
    Count the number of visits (rows in the key card data) per employee_id.
    """
    return (
        df.groupby("employee_id")
        .size()
        .reset_index(name="visit_count")
    )

def calculate_average_arrival_hour(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate the average arrival hour for each employee."""
    df = df.copy()
    
    # Get first scan of each day for each employee
    first_scans = (
        df.sort_values(['employee_id', 'date_only', 'parsed_time'])
        .groupby(['employee_id', 'date_only'])
        .first()
        .reset_index()
    )
    
    # Calculate arrival hour from parsed_time
    first_scans['arrival_hour'] = first_scans['parsed_time'].dt.hour
    
    # Calculate average arrival hour per employee
    return (
        first_scans
        .groupby('employee_id')['arrival_hour']
        .mean()
        .round(2)
        .reset_index()
    )

def calculate_daily_attendance_percentage(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate the daily attendance percentage for hybrid employees.
    Only counts employees who:
    - Are based in London UK
    - Have hybrid working status
    - Were employed on that date (after hire date, before resignation)
    """
    # Get all unique dates and sort them
    all_dates = sorted(df['date_only'].unique())
    
    # Initialize results
    daily_attendance = []
    
    for date in all_dates:
        # Filter for employees who were employed on this date
        active_mask = (
            (pd.to_datetime(df['Combined hire date']) <= date) &
            (
                (df['Most recent day worked'].isna()) |  # Still employed
                (pd.to_datetime(df['Most recent day worked']) >= date)  # Not yet left
            )
        )
        
        # Filter for London & Hybrid employees
        location_mask = (df['Location'] == 'London UK')
        working_mask = (df['Working Status'] == 'Hybrid')
        
        # Get total eligible employees for this date
        eligible_employees = df[
            active_mask & location_mask & working_mask
        ]['employee_id'].nunique()
        
        # Get employees who were present
        present_employees = df[
            (df['date_only'] == date) &
            active_mask & 
            location_mask & 
            working_mask &
            (df['present'] == 'Yes')
        ]['employee_id'].nunique()
        
        # Calculate percentage
        if eligible_employees > 0:
            percentage = (present_employees / eligible_employees) * 100
        else:
            percentage = 0
        
        daily_attendance.append({
            'date': date,
            'total_eligible': eligible_employees,
            'total_present': present_employees,
            'percentage': round(percentage, 1)
        })
    
    return pd.DataFrame(daily_attendance)

def calculate_weekly_attendance_percentage(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate weekly attendance percentage, considering Tuesday-Thursday.
    Uses the attendance table (one row per employee per day).
    """
    # Ensure we're working with datetime
    df = df.copy()
    df['date_only'] = pd.to_datetime(df['date_only'])
    
    # Filter for office days and create week_commencing
    office_days = df[df['day_of_week'].isin(['Tuesday', 'Wednesday', 'Thursday'])]
    office_days['week_commencing'] = office_days['date_only'] - pd.to_timedelta(
        office_days['date_only'].dt.dayofweek, unit='d')
    
    result = []
    for week in sorted(office_days['week_commencing'].unique()):
        week_end = week + pd.Timedelta(days=6)
        
        # Get eligible employees for this week
        eligible_employees = df[
            (df['Location'] == 'London UK') & 
            (df['Working Status'] == 'Hybrid') &
            (pd.to_datetime(df['Combined hire date']) <= week_end) &
            ((pd.to_datetime(df['Most recent day worked']) >= week) | 
             (df['Status'] == 'Active'))
        ].drop_duplicates('employee_id')
        
        if len(eligible_employees) == 0:
            continue
        
        # For each eligible employee, count their attendance days this week
        total_attendance = 0
        for _, emp in eligible_employees.iterrows():
            days_attended = office_days[
                (office_days['week_commencing'] == week) &
                (office_days['employee_id'] == emp['employee_id']) &
                (office_days['present'] == 'Yes')
            ]['date_only'].nunique()
            total_attendance += days_attended
        
        # Total possible days is (eligible employees × 3 days)
        total_possible_days = len(eligible_employees) * 3
        attendance_percentage = (total_attendance / total_possible_days * 100)
        
        result.append({
            'week_commencing': week,
            'days_attended': total_attendance,
            'total_possible_days': total_possible_days,
            'attendance_percentage': attendance_percentage
        })
    
    return pd.DataFrame(result).sort_values('week_commencing')

def calculate_attendance_by_weekday(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate attendance numbers by day of week.
    
    Args:
        df: Combined dataframe with employee and attendance data
        
    Returns:
        DataFrame with attendance by weekday
    """
    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
    weekday_counts = df.groupby('day_of_week').apply(
        lambda x: pd.Series({
            'london_hybrid_count': sum((x['Location'] == 'London UK') & 
                                     (x['Working Status'] == 'Hybrid') &
                                     (x['present'] == 'Yes')),
            'other_count': sum(~((x['Location'] == 'London UK') & 
                               (x['Working Status'] == 'Hybrid')) &
                             (x['present'] == 'Yes'))
        })
    ).reset_index()
    
    weekday_counts['day_of_week'] = pd.Categorical(
        weekday_counts['day_of_week'], 
        categories=weekday_order, 
        ordered=True
    )
    return weekday_counts.sort_values('day_of_week')

def calculate_attendance_by_division(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate attendance numbers and percentages by division.
    Counts unique employee-days for accurate attendance tracking.
    """
    # Get unique divisions, filtering out NaN values
    unique_divisions = [d for d in df['Division'].unique() if pd.notna(d)]
    unique_divisions.sort()  # Sort alphabetically
    
    result = []
    for division in unique_divisions:
        # Get division employees
        division_employees = df[
            (df['Division'] == division) & 
            (df['Location'] == 'London UK') & 
            (df['Working Status'] == 'Hybrid')
        ].drop_duplicates('employee_id')
        
        if len(division_employees) == 0:
            continue
        
        # Count unique employee-days of attendance
        attendance_days = df[
            (df['Division'] == division) & 
            (df['Location'] == 'London UK') & 
            (df['Working Status'] == 'Hybrid') & 
            (df['present'] == 'Yes')
        ].groupby('employee_id')['date_only'].nunique().sum()
        
        # Calculate total possible days for each employee based on their employment period
        total_possible_days = 0
        for _, emp in division_employees.iterrows():
            hire_date = pd.to_datetime(emp['Combined hire date'])
            last_day = pd.to_datetime(emp['Most recent day worked'])
            
            # Get all dates in the dataset
            all_dates = pd.to_datetime(df['date_only'].unique())
            
            # Filter dates to employment period
            valid_dates = all_dates[
                (all_dates >= hire_date) & 
                ((all_dates <= last_day) | (emp['Status'] == 'Active'))
            ]
            
            total_possible_days += len(valid_dates)
        
        # Calculate percentage
        attendance_percentage = (attendance_days / total_possible_days * 100) if total_possible_days > 0 else 0
        
        result.append({
            'division': division,
            'attendance_days': attendance_days,
            'total_possible_days': total_possible_days,
            'attendance_percentage': attendance_percentage
        })
    
    return pd.DataFrame(result)

def calculate_individual_attendance(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate individual employee attendance metrics."""
    result = []
    
    # Ensure Date/time is properly parsed
    df['Date/time'] = pd.to_datetime(df['Date/time'], dayfirst=True)
    df['date_only'] = pd.to_datetime(df['date_only'])
    
    # Add more detailed debugging
    print("\nOverall Dataset Analysis:")
    print(f"Total unique dates: {df['date_only'].nunique()}")
    print(f"Date range: {df['date_only'].min()} to {df['date_only'].max()}")
    print("\nEntrance distribution by hour:")
    print(df['Date/time'].dt.hour.value_counts().sort_index())
    print("\nEntrance distribution by location:")
    print(df['Where'].value_counts())
    
    # Get the date range of our key card data
    data_start_date = df['date_only'].min()
    data_end_date = df['date_only'].max()
    
    # Get unique employees
    unique_employees = df['employee_id'].unique()
    
    for emp_id in unique_employees:
        emp_data = df[df['employee_id'] == emp_id].copy()
        
        # Get employee info
        first_record = emp_data.iloc[0]
        emp_name = first_record['Last name, First name']
        hire_date = pd.to_datetime(first_record['Combined hire date'])
        last_day = pd.to_datetime(first_record['Most recent day worked'])
        location = first_record['Location']
        working_status = first_record['Working Status']
        status = first_record['Status']
        
        # If last_day is NaT (for active employees), use the end of our data range
        if pd.isna(last_day) and status == 'Active':
            last_day = data_end_date
        
        # Total days attended (any day)
        days_attended = emp_data[emp_data['present'] == 'Yes']['date_only'].nunique()
        
        # Initialize core days metrics
        core_days_percentage = None
        avg_entry_time = None
        
        # Only calculate core metrics for London Hybrid employees
        if location == 'London UK' and working_status == 'Hybrid':
            # Get all core days (Tue-Thu) during employment AND within our data range
            start_date = max(hire_date, data_start_date)
            end_date = min(last_day, data_end_date)
            
            all_dates = pd.date_range(start=start_date, end=end_date)
            core_weekdays = all_dates[all_dates.dayofweek.isin([1, 2, 3])]  # Tue=1, Wed=2, Thu=3
            total_possible_core_days = len(core_weekdays)
            
            # Filter for attended core days
            core_attendance = emp_data[
                (emp_data['present'] == 'Yes') & 
                (emp_data['date_only'].dt.dayofweek.isin([1, 2, 3]))
            ]
            
            # Debug: Print sample of core attendance for this employee
            if not core_attendance.empty:
                print(f"\nDebug - Employee {emp_id} ({emp_name}) core attendance sample:")
                print(f"Date/time dtype: {core_attendance['Date/time'].dtype}")
                print("\nFirst 5 records sorted by Date/time:")
                debug_sample = (
                    core_attendance
                    .sort_values('Date/time')
                    .head()
                    [['date_only', 'Date/time', 'Event', 'Where']]
                )
                print(debug_sample)
            
            # Count unique core days attended
            core_days_attended = core_attendance['date_only'].nunique()
            
            # Calculate percentage
            if total_possible_core_days > 0:
                core_days_percentage = round((core_days_attended / total_possible_core_days) * 100, 1)
            
            # Calculate average first entry time for core days
            if not core_attendance.empty:
                # Sort by date/time and get actual first entry of each day
                daily_first_entries = (
                    core_attendance
                    .sort_values('Date/time', ascending=True)
                    .groupby('date_only')['Date/time']
                    .first()
                )
                
                # Flag late entries (after 11:00)
                late_entries = daily_first_entries[daily_first_entries.dt.hour >= 11]
                if not late_entries.empty:
                    print(f"\nLate entries for {emp_name}:")
                    print(late_entries)
                    print(f"Entrance locations for late entries:")
                    print(core_attendance[
                        core_attendance['date_only'].isin(late_entries.index)
                    ]['Where'].value_counts())
                
                # Debug: Print first entries for verification
                print("\nFirst entries of each day:")
                print(daily_first_entries.head())
                
                # Extract just the time component
                daily_times = daily_first_entries.dt.time
                
                # Convert times to minutes since midnight
                minutes = pd.Series([
                    t.hour * 60 + t.minute 
                    for t in daily_times
                ])
                
                # Calculate average and convert back to time string
                avg_minutes = round(minutes.mean())
                hours = avg_minutes // 60
                mins = avg_minutes % 60
                avg_entry_time = f"{int(hours):02d}:{int(mins):02d}"
                
                # Debug: Print time calculation details
                print(f"\nAverage calculation:")
                print(f"Total minutes: {minutes.tolist()}")
                print(f"Average minutes: {avg_minutes}")
                print(f"Calculated time: {avg_entry_time}")
        
        result.append({
            'employee_name': emp_name,
            'days_attended': days_attended,
            'core_days_percentage': core_days_percentage,
            'avg_entry_time': avg_entry_time
        })
    
    return pd.DataFrame(result)


def create_employee_summary(df: pd.DataFrame) -> pd.DataFrame:
    """
    Create employee summary table with attendance metrics.
    
    Args:
        df: Combined dataframe with employee and attendance data
        
    Returns:
        DataFrame with employee attendance summary
    """
    # Ensure date columns are datetime
    df['date_only'] = pd.to_datetime(df['date_only'])
    df['Combined hire date'] = pd.to_datetime(df['Combined hire date'])
    df['Most recent day worked'] = pd.to_datetime(df['Most recent day worked'])
    
    # Get the full date range from the data
    date_range = pd.date_range(
        start=df['date_only'].min(),
        end=df['date_only'].max()
    )
    
    # Initialize results list
    results = []
    
    # Get unique employees
    unique_employees = df.drop_duplicates('employee_id')[
        ['employee_id', 'Last name, First name']
    ]
    
    for _, emp in unique_employees.iterrows():
        emp_id = emp['employee_id']
        emp_name = emp['Last name, First name']
        
        # Get employee's data
        emp_data = df[df['employee_id'] == emp_id].copy()
        if emp_data.empty:
            continue
            
        # Get first record for this employee
        first_record = emp_data.iloc[0]
        
        # Get employment dates
        hire_date = pd.to_datetime(first_record['Combined hire date'])
        last_day = pd.to_datetime(first_record['Most recent day worked'])
        
        # If last_day is NaT (active employee), use the end of our data range
        if pd.isna(last_day):
            last_day = date_range[-1]
            
        # Filter for days when employee was employed
        employed_dates = date_range[
            (date_range >= hire_date) & 
            (date_range <= last_day)
        ]
        
        # Get attended days
        attended_days = emp_data[
            (emp_data['present'] == 'Yes')
        ]['date_only'].nunique()
        
        # Get Tue-Thu metrics
        tue_thu_mask = emp_data['day_of_week'].isin(['Tuesday', 'Wednesday', 'Thursday'])
        attended_tue_thu = emp_data[
            (emp_data['present'] == 'Yes') & 
            tue_thu_mask
        ]['date_only'].nunique()
        
        # Count potential Tue-Thu during employment
        employed_tue_thu = len(employed_dates[employed_dates.dayofweek.isin([1, 2, 3])])
        
        # Calculate average entry time for Tue-Thu
        tue_thu_entries = emp_data[
            (emp_data['present'] == 'Yes') & 
            tue_thu_mask
        ].groupby('date_only')['parsed_time'].min()
        
        avg_entry_time = (
            tue_thu_entries.dt.hour + 
            tue_thu_entries.dt.minute / 60
        ).mean()
        
        # Convert average entry time to string format
        if pd.notna(avg_entry_time):
            hours = int(avg_entry_time)
            minutes = int((avg_entry_time % 1) * 60)
            avg_entry_str = f"{hours:02d}:{minutes:02d}"
        else:
            avg_entry_str = None
        
        results.append({
            'employee_id': emp_id,
            'name': emp_name,
            'total_days_attended': attended_days,
            'tue_thu_days_attended': attended_tue_thu,
            'potential_tue_thu_days': employed_tue_thu,
            'avg_entry_time': avg_entry_str,
            'attendance_rate': round(attended_tue_thu / employed_tue_thu * 100, 1) if employed_tue_thu > 0 else 0
        })
    
    # Convert to DataFrame and sort by attendance rate
    result_df = pd.DataFrame(results).sort_values('attendance_rate', ascending=False)
    
    # Round numeric columns
    numeric_cols = ['total_days_attended', 'tue_thu_days_attended', 'potential_tue_thu_days', 'attendance_rate']
    result_df[numeric_cols] = result_df[numeric_cols].round(1)
    
    # Rename columns to be more readable
    column_mapping = {
        'employee_name': 'Employee Name',
        'days_attended': 'Total Days Attended',
        'core_days_percentage': 'Core Days Attendance %',
        'avg_entry_time': 'Average Arrival Time'
    }
    
    # Format percentage columns
    if 'core_days_percentage' in result_df.columns:
        result_df['core_days_percentage'] = result_df['core_days_percentage'].apply(
            lambda x: f"{x:.1f}%" if pd.notnull(x) else None
        )
    
    # Drop potential_tuesday_thu_days column if it exists
    if 'potential_tuesday_thu_days' in result_df.columns:
        result_df = result_df.drop('potential_tuesday_thu_days', axis=1)
    
    return result_df.rename(columns=column_mapping)

def calculate_tue_thu_attendance_percentage(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate daily attendance percentage, excluding Mon/Fri."""
    df = df.copy()
    
    # Filter for Tue-Thu
    tue_thu_mask = df['day_of_week'].isin(['Tuesday', 'Wednesday', 'Thursday'])
    df = df[tue_thu_mask]
    
    # Get all unique dates and sort them
    all_dates = sorted(df['date_only'].unique())
    
    daily_attendance = []
    for date in all_dates:
        # Filter for employees who were employed on this date
        active_mask = (
            (pd.to_datetime(df['Combined hire date']) <= date) &
            (
                (df['Most recent day worked'].isna()) |
                (pd.to_datetime(df['Most recent day worked']) >= date)
            )
        )
        
        # Filter for London & Hybrid employees
        london_hybrid_mask = (df['Location'] == 'London UK') & (df['Working Status'] == 'Hybrid')
        
        # Calculate metrics
        eligible_employees = df[active_mask & london_hybrid_mask]['employee_id'].nunique()
        present_employees = df[
            (df['date_only'] == date) &
            active_mask & 
            london_hybrid_mask &
            (df['present'] == 'Yes')
        ]['employee_id'].nunique()
        
        # Calculate percentage
        percentage = (present_employees / eligible_employees * 100) if eligible_employees > 0 else 0
        
        daily_attendance.append({
            'date': date,
            'total_eligible': eligible_employees,
            'total_present': present_employees,
            'percentage': round(percentage, 1)
        })
    
    return pd.DataFrame(daily_attendance)

def calculate_daily_attendance_counts(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate daily attendance counts split by employee type."""
    # Calculate attendance for each day
    daily_counts = []
    
    for date in sorted(df['date_only'].unique()):
        date_mask = (df['date_only'] == date)
        
        # Consider employment dates
        active_mask = (
            (pd.to_datetime(df['Combined hire date']) <= date) &
            (
                (df['Most recent day worked'].isna()) |
                (pd.to_datetime(df['Most recent day worked']) >= date)
            )
        )
        
        london_hybrid_mask = (df['Location'] == 'London UK') & (df['Working Status'] == 'Hybrid')
        
        # Count London + Hybrid who were present
        london_hybrid_count = df[
            date_mask & 
            active_mask &
            london_hybrid_mask & 
            (df['present'] == 'Yes')
        ]['employee_id'].nunique()
        
        # Count others who were present
        others_count = df[
            date_mask & 
            active_mask &
            ~london_hybrid_mask & 
            (df['present'] == 'Yes')
        ]['employee_id'].nunique()
        
        # Get total eligible London + Hybrid employees for that date
        total_eligible_london_hybrid = df[
            active_mask &
            london_hybrid_mask
        ]['employee_id'].nunique()
        
        # Calculate percentage based on eligible employees
        attendance_percentage = (
            (london_hybrid_count / total_eligible_london_hybrid * 100)
            if total_eligible_london_hybrid > 0 else 0
        )
        
        daily_counts.append({
            'date': date,
            'day_of_week': pd.Timestamp(date).strftime('%A'),
            'london_hybrid_count': london_hybrid_count,
            'other_count': others_count,
            'eligible_london_hybrid': total_eligible_london_hybrid,  # Added for verification
            'london_hybrid_percentage': round(attendance_percentage, 1),
            'total_attendance': london_hybrid_count + others_count
        })
    
    return pd.DataFrame(daily_counts)

def calculate_weekly_attendance_counts(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate weekly attendance counts split by employee type."""
    df = df.copy()
    
    # Add week start date (Monday)
    df['week_start'] = df['date_only'] - pd.to_timedelta(df['date_only'].dt.dayofweek, unit='d')
    
    weekly_counts = []
    for week_start in sorted(df['week_start'].unique()):
        week_end = week_start + pd.Timedelta(days=6)
        week_mask = (df['date_only'] >= week_start) & (df['date_only'] <= week_end)
        
        # Consider employment dates
        active_mask = (
            (pd.to_datetime(df['Combined hire date']) <= week_end) &
            (
                (df['Most recent day worked'].isna()) |
                (pd.to_datetime(df['Most recent day worked']) >= week_start)
            )
        )
        
        london_hybrid_mask = (df['Location'] == 'London UK') & (df['Working Status'] == 'Hybrid')
        
        # Calculate daily attendance for London+Hybrid
        london_hybrid_daily = df[
            week_mask & 
            active_mask &
            london_hybrid_mask & 
            (df['present'] == 'Yes')
        ].groupby('date_only')['employee_id'].nunique()
        
        # Calculate daily attendance for others
        others_daily = df[
            week_mask & 
            active_mask &
            ~london_hybrid_mask & 
            (df['present'] == 'Yes')
        ].groupby('date_only')['employee_id'].nunique()
        
        # Get total eligible London+Hybrid employees
        total_eligible_london_hybrid = df[
            week_mask &
            active_mask &
            london_hybrid_mask
        ]['employee_id'].nunique()
        
        # Calculate averages
        london_hybrid_avg = london_hybrid_daily.mean() if not london_hybrid_daily.empty else 0
        others_avg = others_daily.mean() if not others_daily.empty else 0
        
        # Calculate attendance percentage
        attendance_percentage = (
            (london_hybrid_avg / total_eligible_london_hybrid * 100)
            if total_eligible_london_hybrid > 0 else 0
        )
        
        weekly_counts.append({
            'week_start': week_start,
            'london_hybrid_avg': round(london_hybrid_avg, 1),
            'other_avg': round(others_avg, 1),
            'eligible_london_hybrid': total_eligible_london_hybrid,
            'london_hybrid_percentage': round(attendance_percentage, 1),
            'total_avg_attendance': round(london_hybrid_avg + others_avg, 1)
        })
    
    return pd.DataFrame(weekly_counts)

def calculate_period_summary(df: pd.DataFrame, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:
    """Calculate attendance summary by weekday for a given period."""
    df = df.copy()
    
    # Filter for date range
    date_mask = (df['date_only'] >= start_date) & (df['date_only'] <= end_date)
    df = df[date_mask]
    
    # Create weekday averages
    weekday_stats = []
    for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']:
        day_mask = (df['day_of_week'] == day)
        london_hybrid_mask = (df['Location'] == 'London UK') & (df['Working Status'] == 'Hybrid')
        
        # Get attendance for this weekday
        london_hybrid_attendance = df[
            day_mask & 
            london_hybrid_mask & 
            (df['present'] == 'Yes')
        ].groupby('date_only')['employee_id'].nunique().mean()
        
        others_attendance = df[
            day_mask & 
            ~london_hybrid_mask & 
            (df['present'] == 'Yes')
        ].groupby('date_only')['employee_id'].nunique().mean()
        
        # Get eligible London+Hybrid employees
        eligible_london_hybrid = df[
            day_mask &
            london_hybrid_mask
        ]['employee_id'].nunique()
        
        # Calculate percentage
        attendance_percentage = (
            (london_hybrid_attendance / eligible_london_hybrid * 100)
            if eligible_london_hybrid > 0 else 0
        )
        
        weekday_stats.append({
            'weekday': day,
            'london_hybrid_count': round(london_hybrid_attendance, 1),
            'other_count': round(others_attendance, 1),
            'attendance_percentage': round(attendance_percentage, 1)
        })
    
    return pd.DataFrame(weekday_stats)
==== END OF FILE: src/data_analysis.py ====

==== START OF FILE: src/data_cleaning.py ====
import pandas as pd

def load_key_card_data(filepath: str) -> pd.DataFrame:
    """Load key card data from CSV file."""
    df = pd.read_csv(filepath)
    print("\nLoaded key card data columns:", df.columns.tolist())
    return df

def load_employee_info(filepath: str) -> pd.DataFrame:
    """Load employee information from CSV file."""
    df = pd.read_csv(filepath)
    print("\nLoaded employee info columns:", df.columns.tolist())
    return df

def compute_combined_hire_date(df: pd.DataFrame) -> pd.DataFrame:
    """Use the earlier of Hire Date and Original Hire Date (if available)."""
    df = df.copy()
    df["Hire Date"] = pd.to_datetime(df["Hire Date"], errors="coerce", dayfirst=True)
    if "Original Hire Date" in df.columns:
        df["Original Hire Date"] = pd.to_datetime(df["Original Hire Date"], errors="coerce", dayfirst=True)
        df["Combined hire date"] = df[["Hire Date", "Original Hire Date"]].min(axis=1)
    else:
        df["Combined hire date"] = df["Hire Date"]
    return df

def compute_most_recent_day_worked(df: pd.DataFrame) -> pd.DataFrame:
    """Prefer Last Day over Resignation Date for departure info."""
    df = df.copy()
    if "Last Day" in df.columns:
        df["Last Day"] = pd.to_datetime(df["Last Day"], errors="coerce", dayfirst=True)
    if "Resignation Date" in df.columns:
        df["Resignation Date"] = pd.to_datetime(df["Resignation Date"], errors="coerce", dayfirst=True)
    
    df["Most recent day worked"] = df.get("Last Day")
    if "Resignation Date" in df.columns:
        mask = df["Most recent day worked"].isna()
        df.loc[mask, "Most recent day worked"] = df.loc[mask, "Resignation Date"]
    return df

def clean_key_card_data(df: pd.DataFrame) -> pd.DataFrame:
    """Clean and preprocess the key card access data."""
    df = df.copy()
    
    # Extract employee_id from 'User' column with improved regex
    if 'User' in df.columns:
        # Extract numeric ID and convert to numeric type
        df['employee_id'] = df['User'].str.extract(r'^(\d+)').astype(float)
        
        print("\nSample of User column data with extracted IDs:")
        sample_df = pd.concat([
            df['User'],
            df['employee_id']
        ], axis=1).head(10)
        print(sample_df)
        
        # Print value counts to see extraction results
        print("\nEmployee ID extraction stats:")
        print(f"Total rows: {len(df)}")
        print(f"Rows with valid IDs: {df['employee_id'].notna().sum()}")
        print(f"Rows without IDs: {df['employee_id'].isna().sum()}")
    
    # Parse Date/time with explicit format
    df['parsed_time'] = pd.to_datetime(
        df['Date/time'],
        format="%d/%m/%Y %H:%M:%S",
        dayfirst=True,
        errors='coerce'
    )
    
    # Create date_only from parsed_time
    df['date_only'] = df['parsed_time'].dt.floor('d')
    
    # Add day of week
    df['day_of_week'] = df['date_only'].dt.strftime('%A')
    
    return df

def clean_employee_info(df: pd.DataFrame) -> pd.DataFrame:
    """Clean and preprocess the employee information."""
    df = df.copy()
    
    # Convert Employee # to employee_id and ensure it's numeric
    df = df.rename(columns={'Employee #': 'employee_id'})
    df['employee_id'] = pd.to_numeric(df['employee_id'], errors='coerce')
    
    print("\nEmployee info stats:")
    print(f"Total employees: {len(df)}")
    print(f"Employees with valid IDs: {df['employee_id'].notna().sum()}")
    print(f"Unique employee IDs: {df['employee_id'].nunique()}")
    
    # Convert date columns
    date_columns = ['Hire Date', 'Original Hire Date', 'Resignation Date', 
                   'Employment Status: Date']
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)
            print(f"Converted {col} to datetime")
    
    # Add computed date columns
    df = compute_combined_hire_date(df)
    df = compute_most_recent_day_worked(df)
    
    # Clean up Working Status
    if 'Working Status' in df.columns:
        df['Working Status'] = df['Working Status'].str.strip()
        print("\nUnique Working Status values:")
        print(df['Working Status'].value_counts())
    
    return df

def merge_key_card_with_employee_info(
    key_card_df: pd.DataFrame,
    employee_df: pd.DataFrame
) -> pd.DataFrame:
    """Merge key card data with employee information."""
    print("\nBefore merge:")
    print("Key card shape:", key_card_df.shape)
    print("Employee shape:", employee_df.shape)
    
    # Both DataFrames should have 'employee_id' column at this point
    if 'employee_id' not in key_card_df.columns or 'employee_id' not in employee_df.columns:
        raise KeyError("Both DataFrames must have 'employee_id' column")
        
    # Ensure employee_id is numeric in both DataFrames
    key_card_df = key_card_df.copy()
    employee_df = employee_df.copy()
    
    # Print sample of IDs before merge
    print("\nFirst few employee IDs from key card data:")
    print(key_card_df[['User', 'employee_id']].head())
    print("\nFirst few employee IDs from employee info:")
    print(employee_df[['employee_id', 'Last name, First name']].head())
    
    # Merge the DataFrames
    merged_df = pd.merge(
        key_card_df,
        employee_df,
        on='employee_id',
        how='left'
    )
    
    # Debug: Check the merge results
    print("\nAfter merge:")
    print("Merged shape:", merged_df.shape)
    print("\nColumns with high null counts (>50%):")
    null_counts = merged_df.isnull().sum()
    high_nulls = null_counts[null_counts > len(merged_df) * 0.5]
    print(high_nulls)
    
    # Print sample of merged data
    print("\nSample of merged data:")
    sample_cols = ['employee_id', 'User', 'Last name, First name', 'Working Status', 'Location']
    print(merged_df[sample_cols].head(10))
    
    return merged_df

def add_time_analysis_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Add additional time-based analysis columns to the DataFrame."""
    df = df.copy()
    
    # Ensure Date/time is datetime
    if not pd.api.types.is_datetime64_any_dtype(df['Date/time']):
        df['Date/time'] = pd.to_datetime(df['Date/time'], dayfirst=True)
    
    # Add hour of day
    df['hour'] = df['Date/time'].dt.hour
    
    # Add time period categories
    df['time_period'] = pd.cut(
        df['hour'],
        bins=[-1, 9, 12, 14, 17, 24],
        labels=['Early Morning', 'Morning', 'Lunch', 'Afternoon', 'Evening']
    )
    
    return df

==== END OF FILE: src/data_cleaning.py ====

==== START OF FILE: src/data_ingestion.py ====
import pandas as pd
from pathlib import Path

def load_key_card_data(filepath: str) -> pd.DataFrame:
    """
    Load key card CSV data.
    Assumes we have columns 'Date/time' and 'User'.
    """
    return pd.read_csv(filepath)

def load_employee_info(filepath: str) -> pd.DataFrame:
    """
    Load employee info CSV data.
    The important column here is 'Employee #'.
    """
    return pd.read_csv(filepath)

if __name__ == "__main__":
    # Example usage
    base_path = Path(__file__).resolve().parent.parent  # go up one level from 'src'
    key_card_path = base_path / "data" / "raw" / "key_card_access.csv"
    employee_info_path = base_path / "data" / "raw" / "employee_info.csv"

    key_card_df = load_key_card_data(str(key_card_path))
    employee_info_df = load_employee_info(str(employee_info_path))

    print("Key card shape:", key_card_df.shape)
    print("Employee info shape:", employee_info_df.shape)

==== END OF FILE: src/data_ingestion.py ====

==== START OF FILE: src/data_visualization.py ====
import plotly.express as px

def plot_arrival_distribution(df: pd.DataFrame):
    fig = px.histogram(df, x='hour', nbins=24, title='Distribution of Arrival Times')
    fig.show()  # or return fig if used in Streamlit / Dash

==== END OF FILE: src/data_visualization.py ====

== END OF PROJECT CODEBASE DUMP ==
